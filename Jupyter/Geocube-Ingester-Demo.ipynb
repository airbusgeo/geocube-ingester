{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocube Ingester Demo\n",
    "\n",
    "-------\n",
    "\n",
    "**Short description**\n",
    "\n",
    "This notebook introduces you to the Geocube Ingester. You will learn how to populate a Geocube using an automatic ingester.\n",
    "\n",
    "\n",
    "The Geocube Ingester is an example of a complete and parallelizable service to feed the Geocube. The user posts an AOI, a time interval and a set of parameters (to compute the output layers). The ingester takes care of everything from the downloading of the products, the computing and its parallelization and the ingestion in the Geocube.\n",
    "\n",
    "It is composed of three services : workflow, downloader and processor. It is connected to a Geocube and has a couple of interfaces to integrate in the user environment. Some implementations of the interfaces are available and the user is free to implement others according to its environment.\n",
    "\n",
    "\n",
    "<img src=\"data/IngesterArchitecture.png\" width=800>\n",
    "\n",
    "-------\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "-------\n",
    "\n",
    "- The Geocube Ingester (github.com/airbusgeo/geocube-ingester.git)\n",
    "- A Scihub account (SCIHUB_USERNAME and SCIHUB_PASSWORD environement variable)\n",
    "- A Geocube server and the parameters to connect (for the purpose of this notebook, GEOCUBE_SERVER and GEOCUBE_CLIENTAPIKEY environment variable)\n",
    "\n",
    "If the Geocube ingester is run in a local environnement:\n",
    "- ESA SNAP >= 8.0 (https://step.esa.int/main/download/snap-download/)\n",
    "\n",
    "-------\n",
    "\n",
    "**Installation**\n",
    "\n",
    "-------\n",
    "\n",
    "Follow the [Geocube Ingester Installation](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD) guide.\n",
    "\n",
    "-------\n",
    "\n",
    "**Start services**\n",
    "\n",
    "-------\n",
    "NB: Geocube server and Messaging must be started before running workflow server.\n",
    "\n",
    "Follow the *Geocube Ingester Installation* guide.\n",
    "\n",
    "Example with golang in local environement:\n",
    "\n",
    "Start Geocube Server (if you don't have any):\n",
    "```bash\n",
    "export GEOCUBE_ROOT=<local Geocube repository>\n",
    "export GEOCUBE_PORT=8081\n",
    "export GEOCUBE_SERVER=127.0.0.1:$GEOCUBE_PORT\n",
    "export LD_LIBRARY_PATH=/gdal/bin\n",
    "\n",
    "cd $GEOCUBE_ROOT/cmd/server/\n",
    "go build\n",
    "./server --local --dbConnection $DB_CONNECTION --port $GEOCUBE_PORT\n",
    "```\n",
    "\n",
    "Start:\n",
    "- [Pubsub emulator](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#pubsub-emulator)\n",
    "- [Downloader service](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#downloader)\n",
    "- [Processor service](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#processor)\n",
    "- [Workflow service](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#workflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Ingester pipeline & payload\n",
    "\n",
    "The ingestion is done in five steps:\n",
    "\n",
    "<img src=\"data/IngesterPipeline.png\" width=800>\n",
    "\n",
    "The input of the ingester is a payload called **Area**. It contains an AOI, a date interval,  parameters defining the raw products, parameters defining the processing and parameters defining the products to be ingested in the Geocube.\n",
    "\n",
    "An example of a payload:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"aoi\":\"DenmarkDemo\",\n",
    "    \"geometry\":{\n",
    "        \"type\": \"MultiPolygon\",\n",
    "        \"coordinates\":\n",
    "        [\n",
    "\t\t\t[\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t[10.061230468750068, 54.88637695312502],\n",
    "\t\t\t\t\t[9.957128906249977, 54.87246093750002],\n",
    "\t\t\t\t\t[9.903906250000063, 54.896630859374994],\n",
    "\t\t\t\t\t[9.80625, 54.90600585937503],\n",
    "\t\t\t\t\t[9.77119140625004, 55.059912109375034],\n",
    "\t\t\t\t\t[9.78125, 55.06904296875001],\n",
    "\t\t\t\t\t[9.830371093750015, 55.05825195312505],\n",
    "\t\t\t\t\t[9.998828125000045, 54.986474609374994],\n",
    "\t\t\t\t\t[10.05771484375006, 54.90791015624998],\n",
    "\t\t\t\t\t[10.061230468750068, 54.88637695312502]\n",
    "\t\t\t\t]\n",
    "\t\t\t]\n",
    "    \t]\n",
    "    },\n",
    "    \"start_time\":\"2021-06-01T00:00:00.000Z\",\n",
    "    \"end_time\":\"2021-06-10T00:00:00.000Z\",\n",
    "\t\"scene_type\":{\n",
    "        \"constellation\":\"sentinel1\",\n",
    "        \"parameters\": {\n",
    "            \"producttype\": \"SLC\",\n",
    "            \"polarisationmode\": \"VV VH\",\n",
    "            \"sensoroperationalmode\": \"IW\",\n",
    "\t\t\t\"relativeorbitnumber\": \"44\"\n",
    "        }\n",
    "    },\n",
    "    \"scene_graph_name\":\"S1Preprocessing\",\n",
    "    \"tile_graph_name\":\"S1BackscatterCoherence\",\n",
    "    \"graph_config\":{\n",
    "        \"projection\":\"EPSG:32632\",\n",
    "        \"snap_cpu_parallelism\":\"8\"\n",
    "    },\n",
    "    \"layers\":{\n",
    "\t\t\"sigma0_VV\": {\"variable\":\"BackscatterSigma0VV\", \"instance\":\"RNKell\"},\n",
    "\t\t\"sigma0_VH\": {\"variable\":\"BackscatterSigma0VH\", \"instance\":\"RNKell\"},\n",
    "\t\t\"coh_VV\": {\"variable\":\"CoherenceVV\", \"instance\":\"master\"},\n",
    "\t\t\"coh_VH\": {\"variable\":\"CoherenceVH\", \"instance\":\"master\"}\n",
    "    },\n",
    "    \"record_tags\":{\n",
    "        \"source\": \"tutorial\",\n",
    "\t\t\"provider\": \"geocube-ingester\",\n",
    "\t\t\"area\":\"Denmark\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In details (all fields are mandatory unless otherwise stated):\n",
    "- `aoi`: Unique name used to identify the Area in the workflow. After a first ingestion, new scenes can be added to the same area, benefiting from automatic scenes reference picking (useful for S1-bursts).\n",
    "- `geometry`: of the aoi\n",
    "- `start_time`, `end_time`: date interval\n",
    "- `scene_type`: describing the type of the products to be downloaded\n",
    "    - `constellation`: Name of the Satellite Constellation (currently supported : sentinel1, sentinel2)\n",
    "    - `parameters`: (optional) specific parameters to filter the results (see Scihub API guide)\n",
    "- `scene_graph_name`: name of the graph that will be used just after downloading the scene (or \"CopyToStorage\")\n",
    "- `tile_graph_name`: name of the graph that will be used to process each tiles (or \"Pass\")\n",
    "- `graph_config`: (optional): specific configuration of the graphs\n",
    "- `layers`: mapping between layers to be indexed in the Geocube and the corresponding variable.instance from the Geocube (see Geocube Documentation).\n",
    "    - `layername: {\"variable\":\"variable_name\", \"instance\":\"instance_name\"}`\n",
    "- `record_tags` (optional): user-defined tags for identifying/creating the record in the Geocube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable & Instance dependencies\n",
    "\n",
    "The processor service will index images referenced by variables and instances.\n",
    "\n",
    "For the purpose of this tutorial, these variables have to be created in the Geocube (Geocube server uri is defined as `GEOCUBE_SERVER` environment variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocube\n",
    "import os\n",
    "from geocube import entities, utils\n",
    "\n",
    "# Change geocube server URI to your Geocube (i.e. 127.0.0.1:8081)\n",
    "LOCAL_GEOCUBE_SERVER = os.environ.get('GEOCUBE_SERVER')\n",
    "\n",
    "# If geocube is not on local environment, you must define CLIENT_APIKEY\n",
    "client = geocube.Client(LOCAL_GEOCUBE_SERVER, False, '')\n",
    "\n",
    "def create(variable, instance, metadata, profile):\n",
    "    try:\n",
    "        client.create_variable(variable, **profile)\n",
    "    except utils.GeocubeError:\n",
    "        pass\n",
    "    try:\n",
    "        client.variable(variable).instantiate(instance, metadata)\n",
    "    except utils.GeocubeError:\n",
    "        pass\n",
    "\n",
    "\n",
    "profile = {'dformat': ('float32', 0, 0, 1), 'bands': [''], 'resampling_alg': entities.Resampling.cubic}\n",
    "profile['description'] = \"Coherence VH - Terrain corrected (SRTM3sec)\"\n",
    "create(\"CoherenceVH\", \"master\", {\"processor\": \"snap8\"}, profile)\n",
    "\n",
    "profile['description'] = \"Coherence VV - Terrain corrected (SRTM3sec)\"\n",
    "create(\"CoherenceVV\", \"master\", {\"processor\": \"snap8\"}, profile)\n",
    "\n",
    "profile['description'] = \"Backscatter VV - Terrain corrected (SRTM3sec)\"\n",
    "create(\"BackscatterSigma0VV\", \"RNKell\", {\"method\": \"Kellndorfer\", \"processor\": \"snap8\"}, profile)\n",
    "\n",
    "profile['description'] = \"Backscatter VH - Terrain corrected (SRTM3sec)\"\n",
    "create(\"BackscatterSigma0VH\", \"RNKell\", {\"method\": \"Kellndorfer\", \"processor\": \"snap8\"}, profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook init\n",
    "\n",
    "Set the URI (including PORT) of the workflow server and init the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "try:\n",
    "    os.mkdir('outputs')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Change workflow URI to your workflow (i.e. 127.0.0.1:8082)\n",
    "workflow_server = os.environ.get('GEOCUBE_INGESTER_WORKFLOW')\n",
    "\n",
    "def json_pretty_print(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        j = json.load(f)\n",
    "    print(json.dumps(j, indent=4, sort_keys=True))\n",
    "    \n",
    "def is_wrong_json(jsonData):\n",
    "    return json_file.read(1) != \"{\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - List scenes\n",
    "\n",
    "The first step of the ingestion is to list the scenes available on the AOI at the given dates.\n",
    "The ingester will query the scenes from a catalogue provider (by default Scihub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET -s -F \"area=@data/DenmarkDemo.json\" {workflow_server}/catalog/scenes > outputs/scenes.json\n",
    "\n",
    "with open(\"outputs/scenes.json\") as json_file:\n",
    "    if is_wrong_json(json_file):\n",
    "        os.remove(\"outputs/scenes.json\")\n",
    "        copyfile(\"data/scenes.json\", \"outputs/scenes.json\")\n",
    "\n",
    "json_pretty_print(\"outputs/scenes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - List tiles\n",
    "Then, the scenes will be divided into tiles. By default, for Sentinel-2, the tile is the whole image and for Sentinel-1, the scenes are divided in bursts. The burst inventory is done using annotations available in the SAFE file. Creodias provides a service to download these annotations files without downloading the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET -s -F \"area=@data/DenmarkDemo.json\" -F \"scenes=@outputs/scenes.json\" {workflow_server}/catalog/tiles > outputs/tiles.json\n",
    "\n",
    "with open(\"outputs/tiles.json\") as json_file:\n",
    "    if is_wrong_json(json_file):\n",
    "        os.remove(\"outputs/tiles.json\")\n",
    "        copyfile(\"data/tiles.json\", \"outputs/tiles.json\")\n",
    "\n",
    "json_pretty_print(\"outputs/tiles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Post Area\n",
    "Then the Area, with scenes and tiles, is posted to the workflow service that is in charge of creating and running the processing flow.\n",
    "\n",
    "Using tiles.json :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -F \"area=@data/DenmarkDemo.json\" -F \"tiles=@outputs/tiles.json\" {workflow_server}/catalog/aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scenes.json :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -F \"area=@data/DenmarkDemo.json\" -F \"scenes=@outputs/scenes.json\" {workflow_server}/catalog/aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -F \"area=@data/DenmarkDemo.json\" {workflow_server}/catalog/aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Monitoring\n",
    "The scenes to be downloaded are sent to the Downloader Service, then the tiles to be processed are sent to the Processor Service. If an autoscaller is configured, the downloading and the processing are done in parallel using all available machines.\n",
    "\n",
    "Some EndPoints are available to monitor this processing-flow.\n",
    "\n",
    "### Aoi info\n",
    "- Overview of the workload for an AOI: `GET: /aoi/{aoi}`\n",
    "- Pretty display of the workflow: `GET: /aoi/{aoi}/dot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl {workflow_server}/aoi/DenmarkDemo\n",
    "!curl -s {workflow_server}/aoi/DenmarkDemo/dot > outputs/DenmarkDemo.dot\n",
    "\n",
    "import graphviz\n",
    "dot = graphviz.Source.from_file('DenmarkDemo.dot', directory=\"outputs\")\n",
    "filename=dot.render(format='png')\n",
    "from IPython.display import Image\n",
    "with open(os.path.join(os.getcwd(), filename),'rb') as f:\n",
    "    display(Image(data=f.read(), format='png', width=1024, height=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Info\n",
    "- List Scenes of an AOI: `GET /aoi/{aoi}/scenes`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/aoi/DenmarkDemo/scenes  > outputs/listScenesFromAOI.json\n",
    "json_pretty_print(\"outputs/listScenesFromAOI.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Scenes of an AOI filtered by Status: `GET /aoi/{aoi}/scenes/{status}` (status in \\[NEW, PENDING, DONE, RETRY, FAILED\\])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/aoi/DenmarkDemo/scenes/PENDING  > outputs/pendingScenes.json\n",
    "pending_scene_id = int\n",
    "with open('outputs/pendingScenes.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    pending_scene_id = data[0]['id']\n",
    "json_pretty_print(\"outputs/pendingScenes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Scene using its id: `GET /scene/{scene}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/scene/{pending_scene_id}  > outputs/scene.json\n",
    "json_pretty_print(\"outputs/scene.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiles Infos\n",
    "- Get Tiles of a Scene: `GET /scene/{scene}/tiles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/scene/{pending_scene_id}/tiles  > outputs/tilesFromScene.json\n",
    "tile_id = int\n",
    "with open('outputs/tilesFromScene.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    tile_id = data[0]['id']\n",
    "json_pretty_print(\"outputs/tilesFromScene.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Tile using its id: `GET /tile/{tile}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/tile/{tile_id}  > outputs/getTiles.json\n",
    "json_pretty_print(\"outputs/getTiles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Tiles of an AOI filtered by Status: `GET /aoi/{aoi}/tiles/{status}` (status in \\[NEW, PENDING, DONE, RETRY, FAILED\\])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/aoi/DenmarkDemo/tiles/NEW  > outputs/tilesFromStatusAOI.json\n",
    "json_pretty_print(\"outputs/tilesFromStatusAOI.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others monitoring endpoints availables\n",
    "- `PUT /scene/{scene}/retry` > retry the scene (iif scene.Status=RETRY)\n",
    "- `PUT /scene/{scene}/fail` > tag the scene and all its tiles as failed and update the graph of dependencies (iif scene.Status=RETRY if `/force` is not stated)\n",
    "\n",
    "\n",
    "- `PUT /tile/{tile}/retry` > retry the tile (iif tile.Status=RETRY)\n",
    "- `PUT /tile/{tile}/fail` > tag the tile as failed and update the graph of dependencies  (iif tile.Status=RETRY if `/force` is not stated)\n",
    "\n",
    "- `POST /aoi/{aoi}` > create a new AOI\n",
    "- `POST /aoi/{aoi}/scene` > add a new scene and its tiles to the graph of dependencies\n",
    "- `PUT /aoi/{aoi}/retry` > retry all the scenes and tiles of the AOI (iif Status=RETRY)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
