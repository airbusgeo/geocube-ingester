{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocube Ingester Demo\n",
    "\n",
    "-------\n",
    "\n",
    "**Short description**\n",
    "\n",
    "This notebook introduces you to the Geocube Ingester. You will learn how to populate a Geocube using an automatic ingester.\n",
    "\n",
    "\n",
    "The Geocube Ingester is an example of a complete and parallelizable service to feed the Geocube. The user posts an AOI, a time interval and a set of parameters (to compute the output layers). The ingester takes care of everything from the downloading of the products, the computing and its parallelization and the ingestion in the Geocube.\n",
    "\n",
    "It is composed of three services : workflow, downloader and processor. It is connected to a Geocube and has a couple of interfaces to integrate in the user environment. Some implementations of the interfaces are available and the user is free to implement others according to its environment.\n",
    "\n",
    "\n",
    "<img src=\"data/IngesterArchitecture.png\" width=800>\n",
    "\n",
    "-------\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "-------\n",
    "\n",
    "- The Geocube Ingester (github.com/airbusgeo/geocube-ingester.git)\n",
    "- A Scihub account (SCIHUB_USERNAME and SCIHUB_PASSWORD environment variable)\n",
    "- A Geocube server and the parameters to connect (for the purpose of this notebook, GEOCUBE_SERVER and GEOCUBE_CLIENTAPIKEY environment variable)\n",
    "\n",
    "If the Geocube ingester is run in a local environment:\n",
    "- ESA SNAP >= 8.0 (https://step.esa.int/main/download/snap-download/)\n",
    "\n",
    "-------\n",
    "\n",
    "**Installation**\n",
    "\n",
    "-------\n",
    "\n",
    "Follow the [Geocube Ingester Installation](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD) guide.\n",
    "\n",
    "-------\n",
    "\n",
    "**Start services**\n",
    "\n",
    "-------\n",
    "\n",
    "NB: Geocube server must be started before running workflow server.\n",
    "If you don't have any Geocube Server running, follow the [Geocube Installation Guide](https://github.com/airbusgeo/geocube/blob/main/INSTALL.MD).\n",
    "You can run a [local geocube server](https://github.com/airbusgeo/geocube/blob/main/INSTALL.MD#docker-compose) using [docker-compose](https://docs.docker.com/compose/).\n",
    "\n",
    "Start services [using docker-compose](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#docker-compose) or one by one:\n",
    "- [Pubsub emulator](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#pubsub-emulator)\n",
    "- [Downloader service](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#downloader)\n",
    "- [Processor service](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#processor)\n",
    "- [Workflow service](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD#workflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Ingester pipeline & payload\n",
    "\n",
    "The ingestion is done in five steps:\n",
    "\n",
    "<img src=\"data/IngesterPipeline.png\" width=800>\n",
    "\n",
    "The input of the ingester is a payload called **Area**. It contains an AOI, a date interval, parameters defining the raw products, parameters defining the processing and parameters defining the products to be ingested in the Geocube.\n",
    "\n",
    "The payload is a GeoJSON (all fields are mandatory unless otherwise stated):\n",
    "- AOI according to GeoJSON standards (`type`, `geometry`, `coordinates`...)\n",
    "- `name`: Unique name used to identify the Area in the workflow. After a first ingestion, new scenes can be added to the same area, benefiting from automatic scenes reference picking (useful for S1-bursts).\n",
    "- `start_time`, `end_time`: date interval\n",
    "- `scene_type`: describing the type of the products to be downloaded\n",
    "    - `constellation`: Name of the Satellite Constellation (currently supported : sentinel1, sentinel2)\n",
    "    - `parameters`: (optional) specific parameters to filter the results (see Scihub API guide)\n",
    "- `scene_graph_name`: name of the graph that will be used just after downloading the scene (or \"CopyToStorage\")\n",
    "- `tile_graph_name`: name of the graph that will be used to process each tiles (or \"Pass\")\n",
    "- `graph_config`: (optional): specific configuration of the graphs\n",
    "- `layers`: mapping between layers to be indexed in the Geocube and the corresponding variable.instance from the Geocube (see Geocube Documentation).\n",
    "    - `layername: {\"variable\":\"variable_name\", \"instance\":\"instance_name\"}`\n",
    "- `record_tags` (optional): user-defined tags for identifying/creating the record in the Geocube.\n",
    "\n",
    "An example of a payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "payloadFile = \"data/DenmarkDemo.json\"\n",
    "with open(payloadFile, \"r\") as f:\n",
    "    j = json.load(f)\n",
    "    print(json.dumps(j, indent=4))\n",
    "payloadName = j[\"name\"]\n",
    "    \n",
    "# Display AOI\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geocube import utils\n",
    "aoi = utils.read_aoi(payloadFile)\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "base = world.plot(color='lightgrey', edgecolor='white')\n",
    "gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=base, edgecolor='black')\n",
    "plt.xlim([aoi.bounds[0]-0.2, aoi.bounds[2]+0.2])\n",
    "plt.ylim([aoi.bounds[1]-0.2, aoi.bounds[3]+0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable & Instance dependencies\n",
    "\n",
    "The processor service will index images referenced by variables and instances.\n",
    "\n",
    "For the purpose of this tutorial, these variables have to be created in the Geocube (Geocube server uri is defined as `GEOCUBE_SERVER` environment variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocube\n",
    "import os\n",
    "from geocube import entities, utils\n",
    "\n",
    "# Define the connection to the server\n",
    "secure = False # in local, or true to use TLS\n",
    "geocube_client_server  = os.environ['GEOCUBE_SERVER']        # e.g. 127.0.0.1:8080 for local use\n",
    "geocube_client_api_key = os.environ['GEOCUBE_CLIENTAPIKEY']  # Usually empty for local use\n",
    "\n",
    "client = geocube.Client(geocube_client_server, secure, geocube_client_api_key)\n",
    "\n",
    "def create(variable, instance, metadata, profile):\n",
    "    try:\n",
    "        client.create_variable(variable, **profile)\n",
    "    except utils.GeocubeError:\n",
    "        pass\n",
    "    try:\n",
    "        client.variable(variable).instantiate(instance, metadata)\n",
    "    except utils.GeocubeError:\n",
    "        pass\n",
    "\n",
    "\n",
    "profile = {'dformat': ('float32', 0, 0, 1), 'bands': [''], 'resampling_alg': entities.Resampling.cubic}\n",
    "profile['description'] = \"Coherence VH - Terrain corrected (SRTM3sec)\"\n",
    "create(\"CoherenceVH\", \"master\", {\"processor\": \"snap8\"}, profile)\n",
    "\n",
    "profile['description'] = \"Coherence VV - Terrain corrected (SRTM3sec)\"\n",
    "create(\"CoherenceVV\", \"master\", {\"processor\": \"snap8\"}, profile)\n",
    "\n",
    "profile['description'] = \"Backscatter VV - Terrain corrected (SRTM3sec)\"\n",
    "create(\"BackscatterSigma0VV\", \"RNKell\", {\"method\": \"Kellndorfer\", \"processor\": \"snap8\"}, profile)\n",
    "\n",
    "profile['description'] = \"Backscatter VH - Terrain corrected (SRTM3sec)\"\n",
    "create(\"BackscatterSigma0VH\", \"RNKell\", {\"method\": \"Kellndorfer\", \"processor\": \"snap8\"}, profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook init\n",
    "\n",
    "Set the URI (including PORT) of the workflow server and init the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    os.mkdir('outputs')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Change workflow URI to your workflow (e.g. 127.0.0.1:8082)\n",
    "workflow_server = os.environ.get('GEOCUBE_INGESTER_WORKFLOW')\n",
    "\n",
    "def json_pretty_print(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        j = json.load(f)\n",
    "    print(json.dumps(j, indent=4, sort_keys=True))\n",
    "    \n",
    "def is_wrong_json(jsonData):\n",
    "    if jsonData.read(1) != \"{\":\n",
    "        warnings.warn(\"Wrong json file: \" + jsonData.read())\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - List scenes\n",
    "\n",
    "The first step of the ingestion is to list the scenes available on the AOI at the given dates.\n",
    "The ingester will query the scenes from a catalogue provider (by default Scihub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET -s -F \"area=@{payloadFile}\" {workflow_server}/catalog/scenes > outputs/scenes.json\n",
    "\n",
    "with open(\"outputs/scenes.json\") as json_file:\n",
    "    if is_wrong_json(json_file):\n",
    "        print(\"Error retrieving scenes. Use backup instead\")\n",
    "        os.remove(\"outputs/scenes.json\")\n",
    "        copyfile(\"data/scenes.json\", \"outputs/scenes.json\")\n",
    "\n",
    "json_pretty_print(\"outputs/scenes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - List tiles\n",
    "Then, the scenes will be divided into tiles. By default, for Sentinel-2, the tile is the whole image and for Sentinel-1, the scenes are divided in bursts. The burst inventory is done using annotations available in the SAFE file. Creodias provides a service to download these annotations files without downloading the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET -s -F \"area=@{payloadFile}\" -F \"scenes=@outputs/scenes.json\" {workflow_server}/catalog/tiles > outputs/tiles.json\n",
    "\n",
    "with open(\"outputs/tiles.json\") as json_file:\n",
    "    if is_wrong_json(json_file):\n",
    "        print(\"Error retrieving scenes. Use backup instead\")\n",
    "        os.remove(\"outputs/tiles.json\")\n",
    "        copyfile(\"data/tiles.json\", \"outputs/tiles.json\")\n",
    "\n",
    "json_pretty_print(\"outputs/tiles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Post Area\n",
    "Then the Area, with scenes and tiles, is posted to the workflow service that is in charge of creating and running the processing flow.\n",
    "\n",
    "Using tiles.json :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -F \"area=@{payloadFile}\" -F \"tiles=@outputs/tiles.json\" {workflow_server}/catalog/aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scenes.json :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -F \"area=@{payloadFile}\" -F \"scenes=@outputs/scenes.json\" {workflow_server}/catalog/aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -F \"area=@{payloadFile}\" {workflow_server}/catalog/aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Monitoring\n",
    "The scenes to be downloaded are sent to the Downloader Service, then the tiles to be processed are sent to the Processor Service. If an autoscaller is configured, the downloading and the processing are done in parallel using all available machines.\n",
    "\n",
    "Some EndPoints are available to monitor this processing-flow.\n",
    "\n",
    "### Aoi info\n",
    "- Overview of the workload for an AOI: `GET: /aoi/{aoi}`\n",
    "- Pretty display of the workflow: `GET: /aoi/{aoi}/dot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl {workflow_server}/aoi/{payloadName}\n",
    "!curl -s {workflow_server}/aoi/{payloadName}/dot > outputs/{payloadName}.dot\n",
    "\n",
    "import graphviz\n",
    "dot = graphviz.Source.from_file(payloadName + '.dot', directory=\"outputs\")\n",
    "filename=dot.render(format='png')\n",
    "from IPython.display import Image\n",
    "with open(os.path.join(os.getcwd(), filename),'rb') as f:\n",
    "    display(Image(data=f.read(), format='png', width=1024, height=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Info\n",
    "- List Scenes of an AOI: `GET /aoi/{aoi}/scenes`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/aoi/{payloadName}/scenes  > outputs/listScenesFromAOI.json\n",
    "json_pretty_print(\"outputs/listScenesFromAOI.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Scenes of an AOI filtered by Status: `GET /aoi/{aoi}/scenes/{status}` (status in \\[NEW, PENDING, DONE, RETRY, FAILED\\])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/aoi/{payloadName}/scenes/PENDING  > outputs/pendingScenes.json\n",
    "pending_scene_id = int\n",
    "with open('outputs/pendingScenes.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    pending_scene_id = data[0]['id']\n",
    "json_pretty_print(\"outputs/pendingScenes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Scene using its id: `GET /scene/{scene}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/scene/{pending_scene_id}  > outputs/scene.json\n",
    "json_pretty_print(\"outputs/scene.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiles Infos\n",
    "- Get Tiles of a Scene: `GET /scene/{scene}/tiles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/scene/{pending_scene_id}/tiles  > outputs/tilesFromScene.json\n",
    "tile_id = int\n",
    "with open('outputs/tilesFromScene.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    tile_id = data[0]['id']\n",
    "json_pretty_print(\"outputs/tilesFromScene.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Tile using its id: `GET /tile/{tile}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/tile/{tile_id}  > outputs/getTiles.json\n",
    "json_pretty_print(\"outputs/getTiles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Tiles of an AOI filtered by Status: `GET /aoi/{aoi}/tiles/{status}` (status in \\[NEW, PENDING, DONE, RETRY, FAILED\\])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/aoi/{payloadName}/tiles/NEW  > outputs/tilesFromStatusAOI.json\n",
    "json_pretty_print(\"outputs/tilesFromStatusAOI.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others monitoring endpoints availables\n",
    "- `PUT /scene/{scene}/retry` > retry the scene (iif scene.Status=RETRY)\n",
    "- `PUT /scene/{scene}/fail` > tag the scene and all its tiles as failed and update the graph of dependencies (iif scene.Status=RETRY if `/force` is not stated)\n",
    "\n",
    "\n",
    "- `PUT /tile/{tile}/retry` > retry the tile (iif tile.Status=RETRY)\n",
    "- `PUT /tile/{tile}/fail` > tag the tile as failed and update the graph of dependencies  (iif tile.Status=RETRY if `/force` is not stated)\n",
    "\n",
    "- `POST /aoi/{aoi}` > create a new AOI\n",
    "- `POST /aoi/{aoi}/scene` > add a new scene and its tiles to the graph of dependencies\n",
    "- `PUT /aoi/{aoi}/retry` > retry all the scenes and tiles of the AOI (iif Status=RETRY)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the results of the ingestion\n",
    "As the aoi is quite small, all the images in the aoi can be retrieved at once.\n",
    "\n",
    "Otherwise, it has to be tiled with, for example, `client.tile_aoi()`. See [Client-Python DataAccess notebook](https://github.com/airbusgeo/geocube-client-python/Jupyter/Geocube-Client-DataAccess.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "plt.rcParams['figure.figsize'] = [60, 15]\n",
    "\n",
    "records = client.list_records(aoi=aoi, tags={\"source\":\"tutorial\", \"constellation\":\"SENTINEL1\"})\n",
    "variables = {\n",
    "    \"BackscatterSigma0VV\": \"RNKell\",\n",
    "    \"BackscatterSigma0VH\": \"RNKell\",\n",
    "    \"CoherenceVV\": \"master\",\n",
    "    \"CoherenceVH\": \"master\"\n",
    "}\n",
    "\n",
    "tile = entities.Tile.from_bbox(gpd.GeoSeries(aoi, crs=4326).to_crs(32632).total_bounds, \"epsg:32632\", resolution=20)\n",
    "\n",
    "for variable, instance in variables.items():\n",
    "    v = client.variable(variable).instance(instance)\n",
    "    cp = entities.CubeParams.from_tile(tile, records=records, instance=v)\n",
    "    fig, axs = plt.subplots(1, len(records), constrained_layout=True)\n",
    "    fig.suptitle(variable, fontsize=60)\n",
    "    for i, (image, metadata, err) in enumerate(client.get_cube_it(cp)):\n",
    "        if err is None:\n",
    "            axs[i].imshow(image[...,0], cmap=\"gray\", vmin=v.dformat.min_value, vmax=v.dformat.max_value)\n",
    "            axs[i].set_title(metadata.min_date, fontsize=40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
