{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Geocube-Ingester Geocube-Ingester is an example of an automatic and parallel ingester for the Geocube . It is provided either as a generic framework for Geocube ingestion or as an example to develop new ingesters. From an AOI, a time interval and a set of parameters (to configure the output layers), the ingester takes care of everything from the downloading of the products, the computing, its parallelization and the ingestion in the Geocube. It currently supports Sentinel-1, Sentinel-2, Landsat8-9, Pl\u00e9iades and SPOT and it's designed to easily add new sources of data or satellites using the interfaces . Dockerfiles are provided to do automatic preprocessing of images using user-defined SNAP-Processing graphs , python script or docker commands .","title":"Home"},{"location":"#geocube-ingester","text":"Geocube-Ingester is an example of an automatic and parallel ingester for the Geocube . It is provided either as a generic framework for Geocube ingestion or as an example to develop new ingesters. From an AOI, a time interval and a set of parameters (to configure the output layers), the ingester takes care of everything from the downloading of the products, the computing, its parallelization and the ingestion in the Geocube. It currently supports Sentinel-1, Sentinel-2, Landsat8-9, Pl\u00e9iades and SPOT and it's designed to easily add new sources of data or satellites using the interfaces . Dockerfiles are provided to do automatic preprocessing of images using user-defined SNAP-Processing graphs , python script or docker commands .","title":"Geocube-Ingester"},{"location":"quickstart/","text":"Getting started This quickstart requires docker-compose . If you don't have a Geocube Server running, you can start one, with docker-compose, following these instructions . Here the quickstart is configured for a geocube server running on localhost:8080, without apikey. Clone https://github.com/airbusgeo/geocube-ingester.git . Start geocube-ingester service with docker-compose, following these instructions . The ingester is running and waiting for your instructions ! Then, you can do the tutorial to learn how to use the ingester.","title":"Getting started"},{"location":"quickstart/#getting-started","text":"This quickstart requires docker-compose . If you don't have a Geocube Server running, you can start one, with docker-compose, following these instructions . Here the quickstart is configured for a geocube server running on localhost:8080, without apikey. Clone https://github.com/airbusgeo/geocube-ingester.git . Start geocube-ingester service with docker-compose, following these instructions . The ingester is running and waiting for your instructions ! Then, you can do the tutorial to learn how to use the ingester.","title":"Getting started"},{"location":"about/development/","text":"Development Status Geocube-Ingester is under development. The API might evolve in backwards incompatible ways until essential functionality is covered. Contributing Contributions are welcome. Please read the contributing guidelines before submitting fixes or enhancements. Licensing Geocube-Ingester is licensed under the Apache License, Version 2.0. See LICENSE for the full license text. Credits Geocube-Ingester is a project under development by Airbus DS Geo SA with the support of CNES .","title":"Development"},{"location":"about/development/#development","text":"","title":"Development"},{"location":"about/development/#status","text":"Geocube-Ingester is under development. The API might evolve in backwards incompatible ways until essential functionality is covered.","title":"Status"},{"location":"about/development/#contributing","text":"Contributions are welcome. Please read the contributing guidelines before submitting fixes or enhancements.","title":"Contributing"},{"location":"about/development/#licensing","text":"Geocube-Ingester is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.","title":"Licensing"},{"location":"about/development/#credits","text":"Geocube-Ingester is a project under development by Airbus DS Geo SA with the support of CNES .","title":"Credits"},{"location":"about/help/","text":"Do not hesitate to ask for help or report issues .","title":"Getting help"},{"location":"about/release-notes/","text":"Release notes 1.1.0 Warning catalog/scenes and catalog/tiles: fields are low case Outdated graphs To fix incoherences or potential limitation in JSON graphs : - Layer \"product\" is replaced by \" product \" - Extension \"all\" is replaced by \" \" - When importing with extension \"zip\", the file is not unzipped (use Extension \" \" to unzip the file) - When importing with extension \"*\", \"SAFE\" or \"dim\", the unzip folder is ensured to have the name . (or just for the former case). Be sure to update the processing graphs, otherwise the behaviour will be undefined. Functionalities Downloader: add a new provider from URL (use --url-provider) Add Documentation \"how to add a new sensor\" Add Landsat AWS Provider (Catalogue&Download) Add Page/Limit to endpoint catalog/scenes Ingester processor: add credentials from KSA to connect to docker Bug fixes LocalProvider does not require that a date is included in the product name GS/FTP: do not try to unzip the downloaded file if the extension is not \"zip\" Docker use ubuntu:noble to fix vulnerabilities OneAtlas catalog: fix bugs. Default OneAtlasUsername is APIKEY Optimization catalog/scenes catalog/tiles: remove unused or empty fields in the JSON file exported + fields are low case Storage: Create zip archive with BestSpeed catalog/aoi: speed-up 1.0.3beta Functionalities AreaToIngest: Add RetryCount to automatically retry a processing or a download RetryCount times AreaToIngest: Add IsRetriable to enable or disable the retry of a processing or a download AreaToIngest: Add StorageURI to define a custom storage URI for downloader and processor Workflow: add endpoint \\aoi{pattern} to list aois by pattern ProcessingGraph: add condition=on_failure/on_fatal_failure and error_condition=... to create/index/delete a file in case of failure remove CreodiasAnnotationsProvider, add UrlAnnotationsProvider instead of GCSAnnotationBucket gs-provider-buckets supports wildcard gs-provider downloads the last version if more than one file is present rename params Catalog parameter --gcstorage -> --gcs-annotations-bucket URL patterns support {KEY}-format to be replaced by information extracted from the scene name (see --help) Add paging for loadScenes & loadTiles Copernicus database catalogue: add --copernicus-catalog Sobloo, Scihub, Onda, Mundi catalog/provider decomissioning Bug fixes return EmptyError, in case of ingestion of an empty area raise an error if scene.AOI != AOI during an ingestion GetDownloadLink from GCSAnnotationsBucket PythonLogFilter does not ignore FATAL ERROR anymore scihub catalogue: retry 3 times oneatlas: exponential retry Processor.index: add more time before retry ingestScenes might delete records that are used somewhere else ingestScenes with more scenes than endDate/startDate POST /catalog/aoi without scenes or tiles Optimization catalog.ScenesToIngest: list all records at once, instead of one by one databse: add status to AOI to retrieve the status more efficiently ingestScenes: by batch instead of one by one","title":"Release Notes"},{"location":"about/release-notes/#release-notes","text":"","title":"Release notes"},{"location":"about/release-notes/#110","text":"","title":"1.1.0"},{"location":"about/release-notes/#warning","text":"catalog/scenes and catalog/tiles: fields are low case","title":"Warning"},{"location":"about/release-notes/#outdated-graphs","text":"To fix incoherences or potential limitation in JSON graphs : - Layer \"product\" is replaced by \" product \" - Extension \"all\" is replaced by \" \" - When importing with extension \"zip\", the file is not unzipped (use Extension \" \" to unzip the file) - When importing with extension \"*\", \"SAFE\" or \"dim\", the unzip folder is ensured to have the name . (or just for the former case). Be sure to update the processing graphs, otherwise the behaviour will be undefined.","title":"Outdated graphs"},{"location":"about/release-notes/#functionalities","text":"Downloader: add a new provider from URL (use --url-provider) Add Documentation \"how to add a new sensor\" Add Landsat AWS Provider (Catalogue&Download) Add Page/Limit to endpoint catalog/scenes Ingester processor: add credentials from KSA to connect to docker","title":"Functionalities"},{"location":"about/release-notes/#bug-fixes","text":"LocalProvider does not require that a date is included in the product name GS/FTP: do not try to unzip the downloaded file if the extension is not \"zip\" Docker use ubuntu:noble to fix vulnerabilities OneAtlas catalog: fix bugs. Default OneAtlasUsername is APIKEY","title":"Bug fixes"},{"location":"about/release-notes/#optimization","text":"catalog/scenes catalog/tiles: remove unused or empty fields in the JSON file exported + fields are low case Storage: Create zip archive with BestSpeed catalog/aoi: speed-up","title":"Optimization"},{"location":"about/release-notes/#103beta","text":"","title":"1.0.3beta"},{"location":"about/release-notes/#functionalities_1","text":"AreaToIngest: Add RetryCount to automatically retry a processing or a download RetryCount times AreaToIngest: Add IsRetriable to enable or disable the retry of a processing or a download AreaToIngest: Add StorageURI to define a custom storage URI for downloader and processor Workflow: add endpoint \\aoi{pattern} to list aois by pattern ProcessingGraph: add condition=on_failure/on_fatal_failure and error_condition=... to create/index/delete a file in case of failure remove CreodiasAnnotationsProvider, add UrlAnnotationsProvider instead of GCSAnnotationBucket gs-provider-buckets supports wildcard gs-provider downloads the last version if more than one file is present rename params Catalog parameter --gcstorage -> --gcs-annotations-bucket URL patterns support {KEY}-format to be replaced by information extracted from the scene name (see --help) Add paging for loadScenes & loadTiles Copernicus database catalogue: add --copernicus-catalog Sobloo, Scihub, Onda, Mundi catalog/provider decomissioning","title":"Functionalities"},{"location":"about/release-notes/#bug-fixes_1","text":"return EmptyError, in case of ingestion of an empty area raise an error if scene.AOI != AOI during an ingestion GetDownloadLink from GCSAnnotationsBucket PythonLogFilter does not ignore FATAL ERROR anymore scihub catalogue: retry 3 times oneatlas: exponential retry Processor.index: add more time before retry ingestScenes might delete records that are used somewhere else ingestScenes with more scenes than endDate/startDate POST /catalog/aoi without scenes or tiles","title":"Bug fixes"},{"location":"about/release-notes/#optimization_1","text":"catalog.ScenesToIngest: list all records at once, instead of one by one databse: add status to AOI to retrieve the status more efficiently ingestScenes: by batch instead of one by one","title":"Optimization"},{"location":"architecture/services/","text":"Process and services The Geocube Ingester is an example of a complete and parallelizable service to feed the Geocube. The user posts an AOI, a time interval and a set of parameters (to compute the output layers). The ingester takes care of everything from the downloading of the products, the computing and its parallelization and the ingestion in the Geocube. It is composed of three services : workflow, downloader and processor. It is connected to a Geocube and has a couple of interfaces to integrate in the user environment. Some implementations of the interfaces are available and the user is free to implement others according to its environment. Process The ingestion is a process in five steps: List the products fitting the criteria Split them in tiles (optional) Create a working graph, with downloading and processing tasks Download the product Process, store and index them in the Geocube Catalogue service The first two steps are done by a service called Catalogue . The goal of the first step is to find all the satellite products ( scenes ) that fit the criterias, sort them by date and group them by localisation. When it's relevant for preprocessing or ingestion, the scenes can be divided into tiles (e.g. S1 images can be split in bursts). This second step is not mandatory. If it's necessary, for example to coregistrate products with the previous one or from a reference , a working graph is created where each tile has an ancestor (the previous product at the same location) and a reference (the first product of the timeserie at the same location). Nevertheless, this working graph can only be executed when the tiles are on the same grid (e.g. granules for S2, bursts for S1... ), hence the interest to use a common grid to divide the scenes into tiles. The catalogue service is usually managed by the workflow service , but it can be standalone. The catalogue service is connected to a Geocube to get, check or record some information to prepare the indexation. Workflow service The workflow service is responsible for the orchestration of the downloading and processing tasks. It is connected to a database that contains all the current and past tasks. Each task has : An ID and a name A status (NEW, PENDING, DONE, RETRY, FAILED) A message (that is especially used to report errors) (Processing tasks only): a reference and a previous Task A processing task is sent to the processing service as soon as its reference and previous tasks are either null, DONE or FAILED. The downloading or processing orders are emitted automatically to the corresponding service through a messaging service. As downloading a product and processing it are two tasks that have not the same requirements in term of CPU, memory, bandwidth, they are executed by two different services. Downloader service The downloader service : has connections to several external providers of images has appropriate rights to store images in a local or near storage. pulls downloading tasks from the messaging service and process them : Download the scenes from several providers . When a list of providers is available, the downloader tries them one by one until the product is found. Split the scenes in tiles (if necessary) according to a processing graph . Upload the tiles on a local or near storage. sent the result (success or failure) to the workflow service through a messaging service. Processor service The processor service : is connected to the Geocube Server (to index the layers processed) has appropriate rights to read images from the local or near storage (the one where the products are downloader) has appropriate rights to write images on a store reachable by the Geocube Server. pulls processing tasks from the messaging service and processes them: Get the tiles from the local storage and process them to as many layers as required, according to a processing graph . Upload the output layers to a storage accessible by the Geocube Index the output layers in the Geocube Clean the temporary files that are stored in the local or near storage sent the result (success or failure) to the workflow service through a messaging service.","title":"Services"},{"location":"architecture/services/#process-and-services","text":"The Geocube Ingester is an example of a complete and parallelizable service to feed the Geocube. The user posts an AOI, a time interval and a set of parameters (to compute the output layers). The ingester takes care of everything from the downloading of the products, the computing and its parallelization and the ingestion in the Geocube. It is composed of three services : workflow, downloader and processor. It is connected to a Geocube and has a couple of interfaces to integrate in the user environment. Some implementations of the interfaces are available and the user is free to implement others according to its environment.","title":"Process and services"},{"location":"architecture/services/#process","text":"The ingestion is a process in five steps: List the products fitting the criteria Split them in tiles (optional) Create a working graph, with downloading and processing tasks Download the product Process, store and index them in the Geocube","title":"Process"},{"location":"architecture/services/#catalogue-service","text":"The first two steps are done by a service called Catalogue . The goal of the first step is to find all the satellite products ( scenes ) that fit the criterias, sort them by date and group them by localisation. When it's relevant for preprocessing or ingestion, the scenes can be divided into tiles (e.g. S1 images can be split in bursts). This second step is not mandatory. If it's necessary, for example to coregistrate products with the previous one or from a reference , a working graph is created where each tile has an ancestor (the previous product at the same location) and a reference (the first product of the timeserie at the same location). Nevertheless, this working graph can only be executed when the tiles are on the same grid (e.g. granules for S2, bursts for S1... ), hence the interest to use a common grid to divide the scenes into tiles. The catalogue service is usually managed by the workflow service , but it can be standalone. The catalogue service is connected to a Geocube to get, check or record some information to prepare the indexation.","title":"Catalogue service"},{"location":"architecture/services/#workflow-service","text":"The workflow service is responsible for the orchestration of the downloading and processing tasks. It is connected to a database that contains all the current and past tasks. Each task has : An ID and a name A status (NEW, PENDING, DONE, RETRY, FAILED) A message (that is especially used to report errors) (Processing tasks only): a reference and a previous Task A processing task is sent to the processing service as soon as its reference and previous tasks are either null, DONE or FAILED. The downloading or processing orders are emitted automatically to the corresponding service through a messaging service. As downloading a product and processing it are two tasks that have not the same requirements in term of CPU, memory, bandwidth, they are executed by two different services.","title":"Workflow service"},{"location":"architecture/services/#downloader-service","text":"The downloader service : has connections to several external providers of images has appropriate rights to store images in a local or near storage. pulls downloading tasks from the messaging service and process them : Download the scenes from several providers . When a list of providers is available, the downloader tries them one by one until the product is found. Split the scenes in tiles (if necessary) according to a processing graph . Upload the tiles on a local or near storage. sent the result (success or failure) to the workflow service through a messaging service.","title":"Downloader service"},{"location":"architecture/services/#processor-service","text":"The processor service : is connected to the Geocube Server (to index the layers processed) has appropriate rights to read images from the local or near storage (the one where the products are downloader) has appropriate rights to write images on a store reachable by the Geocube Server. pulls processing tasks from the messaging service and processes them: Get the tiles from the local storage and process them to as many layers as required, according to a processing graph . Upload the output layers to a storage accessible by the Geocube Index the output layers in the Geocube Clean the temporary files that are stored in the local or near storage sent the result (success or failure) to the workflow service through a messaging service.","title":"Processor service"},{"location":"developer-guide/catalog/","text":"Catalogue NB: This documentation is for developer that want to implement a new Catalogue Provider. For documentation on how to use the catalogue, see User-Guide/Catalogue . Add a new Catalogue In file catalog/scenes.go , add newProvider instantiation in ScenesInventory() method. For a new constellation/satellite: in file catalog/common/naming.go add the constellation name and modify GetConstellationFromString method. Add configuration parameters (credentials, endpoint) in cmd/catalog/main.go and cmd/workflow/main.go . Implement new catalog in interface/catalog following the interface: type ScenesProvider interface { Supports(c common.Constellation) bool SearchScenes(ctx context.Context, area *entities.AreaToIngest, aoi geos.Geometry) (entities.Scenes, error) } SearchScenes method returns a list of available scenes Update the documentation: docs/user-guide/catalog.md to describe the new catalogue and explain how to configure it. docs/user-guide/payload.md to describe the specific parameters to set in the payload-file to request this catalogue.","title":"Catalogue"},{"location":"developer-guide/catalog/#catalogue","text":"NB: This documentation is for developer that want to implement a new Catalogue Provider. For documentation on how to use the catalogue, see User-Guide/Catalogue .","title":"Catalogue"},{"location":"developer-guide/catalog/#add-a-new-catalogue","text":"In file catalog/scenes.go , add newProvider instantiation in ScenesInventory() method. For a new constellation/satellite: in file catalog/common/naming.go add the constellation name and modify GetConstellationFromString method. Add configuration parameters (credentials, endpoint) in cmd/catalog/main.go and cmd/workflow/main.go . Implement new catalog in interface/catalog following the interface: type ScenesProvider interface { Supports(c common.Constellation) bool SearchScenes(ctx context.Context, area *entities.AreaToIngest, aoi geos.Geometry) (entities.Scenes, error) } SearchScenes method returns a list of available scenes Update the documentation: docs/user-guide/catalog.md to describe the new catalogue and explain how to configure it. docs/user-guide/payload.md to describe the specific parameters to set in the payload-file to request this catalogue.","title":"Add a new Catalogue"},{"location":"developer-guide/interfaces/","text":"Interfaces To integrate into the environment of deployment, the ingester has an interface layer. Some implementations of this layer are available and the user is free to implement others depending on its own environment. Messaging The messaging interface is available here : vendor/github.com/geocube/interface/messaging/ . It is used to communicate between workflow , downloader and processor service. It is configured in the corresponding main.go in the cmd folder. Postgres-based implementation A messaging interface based on postgres is implemented using the btubbs/pgq library: vendor/github.com/geocube/interface/messaging/pgqueue . This implementation has autoscaling capabilities. The postgreqsl database must be configured to accept as many connections as the number of workers, because pgqueue keeps the connection open during the whole processing. PubSub implementation Ingester supports PubSub (Google Cloud Platform) messaging broker : vendor/github.com/geocube/interface/messaging/pubsub . Three topics/subscriptions must be created: To communicate from workflow to downloader service i.e : ingester-downloader To communicate from workflow to processor service i.e : ingester-processor To communicate from downloader & processor to workflow i.e : ingester-event NB: Topics & Subscriptions must be created before running Downloader, Processor and Workflow. A Pub/Sub emulator is available to use PubSub in a local system (with limited capacities). Please follow the documentation to install and start the emulator. Storage The storage is used to download products and to store intermediate images and the final images that are indexed in the Geocube. It must be accessible in reading and writing. The interface is available in vendor/github.com/geocube/interface/storage package. Please, refer to the Geocube Installation Guide to implement another interface. Currently supported storages The ingester supports two storage systems: GCS and filesystem. Database The database interface is available here : interface/database/db.go . It is used by the Workflow as a parameter of the service and it is configured in the following file: cmd/workflow/main.go . PostgreSQL implementation Ingester currently supports a Postgresql database : interface/database/pg/ Create a database and run the installation SQL script in order to create all tables, schemas and roles. This script is available in Geocube code source in interface/database/pg/db.sql $ psql -h <database_host> -d <database_name> -f interface/database/pg/db.sql Image Provider To download images from data-storages, the ingester has the current interface: interface/provider/provider.go . It is used by the Downloader service and configured in cmd/downloader/main.go . Current Implementations The Downloader service is currently able to download products from different provider listed here . Depending on the provider, the user may need credentials. Please refer to the API documentation of the provider. Add a new provider See Developer Guide/Provider . Image Catalog To be able to list the scenes available over an AOI, the Ingester has an interface to an external catalogue service : interface/catalog/catalog.go Current implementations The Ingester is currently able to connect to the catalogues defined here Add a new catalogue See Developer Guide/Provider .","title":"Interfaces"},{"location":"developer-guide/interfaces/#interfaces","text":"To integrate into the environment of deployment, the ingester has an interface layer. Some implementations of this layer are available and the user is free to implement others depending on its own environment.","title":"Interfaces"},{"location":"developer-guide/interfaces/#messaging","text":"The messaging interface is available here : vendor/github.com/geocube/interface/messaging/ . It is used to communicate between workflow , downloader and processor service. It is configured in the corresponding main.go in the cmd folder.","title":"Messaging"},{"location":"developer-guide/interfaces/#postgres-based-implementation","text":"A messaging interface based on postgres is implemented using the btubbs/pgq library: vendor/github.com/geocube/interface/messaging/pgqueue . This implementation has autoscaling capabilities. The postgreqsl database must be configured to accept as many connections as the number of workers, because pgqueue keeps the connection open during the whole processing.","title":"Postgres-based implementation"},{"location":"developer-guide/interfaces/#pubsub-implementation","text":"Ingester supports PubSub (Google Cloud Platform) messaging broker : vendor/github.com/geocube/interface/messaging/pubsub . Three topics/subscriptions must be created: To communicate from workflow to downloader service i.e : ingester-downloader To communicate from workflow to processor service i.e : ingester-processor To communicate from downloader & processor to workflow i.e : ingester-event NB: Topics & Subscriptions must be created before running Downloader, Processor and Workflow. A Pub/Sub emulator is available to use PubSub in a local system (with limited capacities). Please follow the documentation to install and start the emulator.","title":"PubSub implementation"},{"location":"developer-guide/interfaces/#storage","text":"The storage is used to download products and to store intermediate images and the final images that are indexed in the Geocube. It must be accessible in reading and writing. The interface is available in vendor/github.com/geocube/interface/storage package. Please, refer to the Geocube Installation Guide to implement another interface.","title":"Storage"},{"location":"developer-guide/interfaces/#currently-supported-storages","text":"The ingester supports two storage systems: GCS and filesystem.","title":"Currently supported storages"},{"location":"developer-guide/interfaces/#database","text":"The database interface is available here : interface/database/db.go . It is used by the Workflow as a parameter of the service and it is configured in the following file: cmd/workflow/main.go .","title":"Database"},{"location":"developer-guide/interfaces/#postgresql-implementation","text":"Ingester currently supports a Postgresql database : interface/database/pg/ Create a database and run the installation SQL script in order to create all tables, schemas and roles. This script is available in Geocube code source in interface/database/pg/db.sql $ psql -h <database_host> -d <database_name> -f interface/database/pg/db.sql","title":"PostgreSQL implementation"},{"location":"developer-guide/interfaces/#image-provider","text":"To download images from data-storages, the ingester has the current interface: interface/provider/provider.go . It is used by the Downloader service and configured in cmd/downloader/main.go .","title":"Image Provider"},{"location":"developer-guide/interfaces/#current-implementations","text":"The Downloader service is currently able to download products from different provider listed here . Depending on the provider, the user may need credentials. Please refer to the API documentation of the provider.","title":"Current Implementations"},{"location":"developer-guide/interfaces/#add-a-new-provider","text":"See Developer Guide/Provider .","title":"Add a new provider"},{"location":"developer-guide/interfaces/#image-catalog","text":"To be able to list the scenes available over an AOI, the Ingester has an interface to an external catalogue service : interface/catalog/catalog.go","title":"Image Catalog"},{"location":"developer-guide/interfaces/#current-implementations_1","text":"The Ingester is currently able to connect to the catalogues defined here","title":"Current implementations"},{"location":"developer-guide/interfaces/#add-a-new-catalogue","text":"See Developer Guide/Provider .","title":"Add a new catalogue"},{"location":"developer-guide/providers/","text":"Providers NB: This documentation is for developer that want to implement a new Image Provider. For documentation on how to use the image providers, see User-Guide/Providers . Add a new provider Add configuration parameters (credentials, endpoint) in cmd/downloader/main.go and add the new provider to the list of providers. Implement the new provider in \u00ecnterface/provider with methods: Name() string Download(ctx context.Context, scene common.Scene, localDir string) error Update the documentation docs/user-guide/providers.md","title":"Provider"},{"location":"developer-guide/providers/#providers","text":"NB: This documentation is for developer that want to implement a new Image Provider. For documentation on how to use the image providers, see User-Guide/Providers .","title":"Providers"},{"location":"developer-guide/providers/#add-a-new-provider","text":"Add configuration parameters (credentials, endpoint) in cmd/downloader/main.go and add the new provider to the list of providers. Implement the new provider in \u00ecnterface/provider with methods: Name() string Download(ctx context.Context, scene common.Scene, localDir string) error Update the documentation docs/user-guide/providers.md","title":"Add a new provider"},{"location":"developer-guide/sensors/","text":"Sensors Add a new sensor Add the name of the constellation (or the name of the sensor) to the list of Constellation in common/naming.go . Call go generate ./... Update the functions of common/naming.go . In catalog/catalog.go , adapt DoTilesInventory method in order to be able to interpret the new constellation. The next step might be to add a catalogue provider or an image provider for this sensor.","title":"Sensor"},{"location":"developer-guide/sensors/#sensors","text":"","title":"Sensors"},{"location":"developer-guide/sensors/#add-a-new-sensor","text":"Add the name of the constellation (or the name of the sensor) to the list of Constellation in common/naming.go . Call go generate ./... Update the functions of common/naming.go . In catalog/catalog.go , adapt DoTilesInventory method in order to be able to interpret the new constellation. The next step might be to add a catalogue provider or an image provider for this sensor.","title":"Add a new sensor"},{"location":"installation/docker-install/","text":"Docker Installation Local docker engine All dockerfile are available. You can build docker images: $ docker build -f cmd/downloader/Dockerfile -t $DOWNLOADER_IMAGE . $ docker build -f cmd/processor/Dockerfile -t $PROCESSOR_IMAGE . $ docker build -f cmd/workflow/Dockerfile -t $WF_IMAGE . You can run docker run command in order to start the application. For more information concerning running option, see: https://docs.docker.com/engine/reference/commandline/run/ Examples with pgqueue: $ export DB_CONNECTION=postgresql://user:password@localhost:5432/ingester?binary_parameters=yes $ export WORKFLOW_PORT=8082 $ export STORAGE=/ingester-storage $ docker run --rm --network=host $WF_IMAGE --pgq-connection=$DB_CONNECTION --event-queue ingester-events --downloader-queue ingester-downloader --processor-queue ingester-processor --db-connection=$DB_CONNECTION --port $WORKFLOW_PORT --geocube-server $GEOCUBE_SERVER $ docker run --rm --network=host -v $STORAGE:$STORAGE $DOWNLOADER_IMAGE --pgq-connection=$DB_CONNECTION --job-queue ingester-downloader --event-queue ingester-events --local-path $STORAGE/products --storage-uri $STORAGE --workdir /tmp -gs-provider-buckets=Sentinel2:gs://gcp-public-data-sentinel-2/tiles/{LATITUDE_BAND}/{GRID_SQUARE}/{GRANULE_ID}/{SCENE}.SAFE --copernicus-username=$COPERNICUS_USERNAME --copernicus-password=$COPERNICUS_PASSWORD $ docker run --rm --network=host -v $STORAGE:$STORAGE $PROCESSOR_IMAGE --pgq-connection=$DB_CONNECTION --job-queue ingester-processor --event-queue ingester-events --geocube-server $GEOCUBE_SERVER --storage-uri $STORAGE --workdir /tmp Docker compose A docker-compose file is provided as example. It's a minimal example, so feel free to edit it to take advantage of the full power of the Geocube-Ingester. Copy the ./cmd/dockerfiles/.env.example to ./cmd/dockerfiles/.env Edit ./docker/.env to set the env variables. Build the base image cd cmd/dockerfiles and docker-compose up","title":"Docker"},{"location":"installation/docker-install/#docker-installation","text":"","title":"Docker Installation"},{"location":"installation/docker-install/#local-docker-engine","text":"All dockerfile are available. You can build docker images: $ docker build -f cmd/downloader/Dockerfile -t $DOWNLOADER_IMAGE . $ docker build -f cmd/processor/Dockerfile -t $PROCESSOR_IMAGE . $ docker build -f cmd/workflow/Dockerfile -t $WF_IMAGE . You can run docker run command in order to start the application. For more information concerning running option, see: https://docs.docker.com/engine/reference/commandline/run/ Examples with pgqueue: $ export DB_CONNECTION=postgresql://user:password@localhost:5432/ingester?binary_parameters=yes $ export WORKFLOW_PORT=8082 $ export STORAGE=/ingester-storage $ docker run --rm --network=host $WF_IMAGE --pgq-connection=$DB_CONNECTION --event-queue ingester-events --downloader-queue ingester-downloader --processor-queue ingester-processor --db-connection=$DB_CONNECTION --port $WORKFLOW_PORT --geocube-server $GEOCUBE_SERVER $ docker run --rm --network=host -v $STORAGE:$STORAGE $DOWNLOADER_IMAGE --pgq-connection=$DB_CONNECTION --job-queue ingester-downloader --event-queue ingester-events --local-path $STORAGE/products --storage-uri $STORAGE --workdir /tmp -gs-provider-buckets=Sentinel2:gs://gcp-public-data-sentinel-2/tiles/{LATITUDE_BAND}/{GRID_SQUARE}/{GRANULE_ID}/{SCENE}.SAFE --copernicus-username=$COPERNICUS_USERNAME --copernicus-password=$COPERNICUS_PASSWORD $ docker run --rm --network=host -v $STORAGE:$STORAGE $PROCESSOR_IMAGE --pgq-connection=$DB_CONNECTION --job-queue ingester-processor --event-queue ingester-events --geocube-server $GEOCUBE_SERVER --storage-uri $STORAGE --workdir /tmp","title":"Local docker engine"},{"location":"installation/docker-install/#docker-compose","text":"A docker-compose file is provided as example. It's a minimal example, so feel free to edit it to take advantage of the full power of the Geocube-Ingester. Copy the ./cmd/dockerfiles/.env.example to ./cmd/dockerfiles/.env Edit ./docker/.env to set the env variables. Build the base image cd cmd/dockerfiles and docker-compose up","title":"Docker compose"},{"location":"installation/k8s-install/","text":"Installation - Kubernetes Cluster Prerequisites Kubctl must be installed and configured in order to be connected to the right kubernetes cluster. You also need database and Messaging (kubernetes examples are available in Geocube Installation Guide) these same examples can be used here. Ingester should be deployed in the same cluster as Geocube, in a different namespace. IAM & Security All the notions of security and service account are not covered in this document. It is the responsibility of the installers. The files presented below are available as examples/templates. They do not present any notions of security. Container Registry You can create your own registry server: https://docs.docker.com/registry/deploying/ Private Registry You can configure your kubernetes deployment files with private docker registry. For more information, see: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ imagePullSecrets is defined in your kubernetes configuration files and image name is specified as follow ex: image: geocube-private-image:tag Docker Hub In case the images are stored on https://hub.docker.com, you can define them as follows in your kubernetes configuration files (postgresql example: image: postgres:11 ): apiVersion: networking.k8s.io/v1 kind: Deployment metadata: name: postgresql spec: replicas: 1 template: spec: containers: - name: postgresql image: postgres:11 In this example, https://hub.docker.com/layers/postgres/library/postgres/11.0/images/sha256-05f9b83f85bdf0382b1cb8fb72d17d7c8098b0287d7dd1df4ff09aa417a0500b?context=explore image will be loaded. Create Namespace You must define a CLUSTER variable environment. The dedicated namespace ingester is created by running the following command: $ kubectl apply -f deploy/k8s/namespace.yaml Create Secrets in namespace Kubernetes configuration file is available here: deploy/k8s/secrets.yaml . All the parameters between {{}} are mandatory: 1. {{GEOCUBE_SERVER}} : uri of the Geocube (eg. 127.0.0.1:8080 ) 2. {{STORAGE_URI}} : uri where to store the outputs (layers) of the Ingester (eg. /ingester/ or gs://ingester/ ) 3. {{DB_CONNECTION}} : uri of the database (eg. postgresql://user:password@localhost:5432/geocube ) The other parameters (mainly authentication information for image providers) are optional. Ingester server must have sufficient rights in order to read and write into database. For more information, see: https://www.postgresql.org/docs/11/auth-pg-hba-conf.html After that, secrets can be created: $ kubectl apply -f deploy/k8s/public/secrets.yaml Create deployment in namespace In order to start Ingester, you have to define some parameters in deploy/k8s/public/workflow.yaml (all the parameters between {{}} are mandatory): {{WORKFLOW_IMAGE}} : Workflow docker image (eg. <container_registry>/processor:<tag> ) {{DOWNLOADER_IMAGE}} : Downloader docker image (eg. <container_registry>/downloader:<tag> ) {{PROCESSOR_IMAGE}} : Processor docker image (eg. <container_registry>/processor:<tag> ) Then, deployement can be created in namespace ingester by running the following command: $ kubectl apply -f deploy/k8s/public/workflow.yaml NB: Workflow.yaml kubernetes file is an example of Ingester deployment in the cloud. You need to adapt them in order to configure database, messaging and storage access. In you want to use pubsub emulator, you need to add PUB_SUB_EMULATOR variable environment in your deployment and replication controller (already describe in Geocube Documentation). If you want to use pgqueue, you need to refer to: PGQueue Configuration Ex configuration with pubSub emulator: env: - name: PUBSUB_EMULATOR_HOST value: 0.0.0.0:8085","title":"Deploying with Kubernetes"},{"location":"installation/k8s-install/#installation-kubernetes-cluster","text":"","title":"Installation - Kubernetes Cluster"},{"location":"installation/k8s-install/#prerequisites","text":"Kubctl must be installed and configured in order to be connected to the right kubernetes cluster. You also need database and Messaging (kubernetes examples are available in Geocube Installation Guide) these same examples can be used here. Ingester should be deployed in the same cluster as Geocube, in a different namespace.","title":"Prerequisites"},{"location":"installation/k8s-install/#iam-security","text":"All the notions of security and service account are not covered in this document. It is the responsibility of the installers. The files presented below are available as examples/templates. They do not present any notions of security.","title":"IAM &amp; Security"},{"location":"installation/k8s-install/#container-registry","text":"You can create your own registry server: https://docs.docker.com/registry/deploying/","title":"Container Registry"},{"location":"installation/k8s-install/#private-registry","text":"You can configure your kubernetes deployment files with private docker registry. For more information, see: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ imagePullSecrets is defined in your kubernetes configuration files and image name is specified as follow ex: image: geocube-private-image:tag","title":"Private Registry"},{"location":"installation/k8s-install/#docker-hub","text":"In case the images are stored on https://hub.docker.com, you can define them as follows in your kubernetes configuration files (postgresql example: image: postgres:11 ): apiVersion: networking.k8s.io/v1 kind: Deployment metadata: name: postgresql spec: replicas: 1 template: spec: containers: - name: postgresql image: postgres:11 In this example, https://hub.docker.com/layers/postgres/library/postgres/11.0/images/sha256-05f9b83f85bdf0382b1cb8fb72d17d7c8098b0287d7dd1df4ff09aa417a0500b?context=explore image will be loaded.","title":"Docker Hub"},{"location":"installation/k8s-install/#create-namespace","text":"You must define a CLUSTER variable environment. The dedicated namespace ingester is created by running the following command: $ kubectl apply -f deploy/k8s/namespace.yaml","title":"Create Namespace"},{"location":"installation/k8s-install/#create-secrets-in-namespace","text":"Kubernetes configuration file is available here: deploy/k8s/secrets.yaml . All the parameters between {{}} are mandatory: 1. {{GEOCUBE_SERVER}} : uri of the Geocube (eg. 127.0.0.1:8080 ) 2. {{STORAGE_URI}} : uri where to store the outputs (layers) of the Ingester (eg. /ingester/ or gs://ingester/ ) 3. {{DB_CONNECTION}} : uri of the database (eg. postgresql://user:password@localhost:5432/geocube ) The other parameters (mainly authentication information for image providers) are optional. Ingester server must have sufficient rights in order to read and write into database. For more information, see: https://www.postgresql.org/docs/11/auth-pg-hba-conf.html After that, secrets can be created: $ kubectl apply -f deploy/k8s/public/secrets.yaml","title":"Create Secrets in namespace"},{"location":"installation/k8s-install/#create-deployment-in-namespace","text":"In order to start Ingester, you have to define some parameters in deploy/k8s/public/workflow.yaml (all the parameters between {{}} are mandatory): {{WORKFLOW_IMAGE}} : Workflow docker image (eg. <container_registry>/processor:<tag> ) {{DOWNLOADER_IMAGE}} : Downloader docker image (eg. <container_registry>/downloader:<tag> ) {{PROCESSOR_IMAGE}} : Processor docker image (eg. <container_registry>/processor:<tag> ) Then, deployement can be created in namespace ingester by running the following command: $ kubectl apply -f deploy/k8s/public/workflow.yaml NB: Workflow.yaml kubernetes file is an example of Ingester deployment in the cloud. You need to adapt them in order to configure database, messaging and storage access. In you want to use pubsub emulator, you need to add PUB_SUB_EMULATOR variable environment in your deployment and replication controller (already describe in Geocube Documentation). If you want to use pgqueue, you need to refer to: PGQueue Configuration Ex configuration with pubSub emulator: env: - name: PUBSUB_EMULATOR_HOST value: 0.0.0.0:8085","title":"Create deployment in namespace"},{"location":"installation/local-install/","text":"Local installation Environment of development Name Version link Golang >= 1.23 https://golang.org/doc/install GDAL >= 3.8 https://gdal.org Python >= 3.7 https://www.python.org/downloads/ PostgreSQL >= 15 https://www.postgresql.org/download/ Docker NC https://docs.docker.com/engine/install/ (ESA SNAP) >=8.0 https://step.esa.int/main/download/snap-download/ Build and run Go application Messaging broker PGQueue To use this messaging broker, create the pgq_jobs table in your postgres database using the following script vendor/github.com/airbusgeo/geocube/interface/messaging/pgqueue/create_table.sql . $ psql -h <database_host> -d <database_name> -f vendor/github.com/airbusgeo/geocube/interface/messaging/pgqueue/create_table.sql Then, start the services the following arguments: - --pgq-connection : connection uri to the postgres database (e.g. postgresql://user:password@localhost:5432/geocube ) - --event-queue events - --downloader-queue/--job-queue downloader - --processor-queue/--job-queue processor PubSub Emulator A Pub/Sub emulator is available to use PubSub in a local system (with limited capacities). Please follow the documentation to start the emulator. Example: gcloud beta emulators pubsub start --project=geocube-emulator --host-port $PUBSUB_EMULATOR_HOST After starting pubsub emulator server, the following script creates ingester topics and subscriptions: tools/pubsub_emulator/main.go . $ go run tools/pubsub_emulator/main.go --project geocube-emulator 2021/06/16 14:26:06 New client for project geocube-emulator 2021/06/16 14:26:06 Create Topic : ingester-downloader 2021/06/16 14:26:06 Create Topic : ingester-processor 2021/06/16 14:26:06 Create Topic : ingester-events 2021/06/16 14:26:06 Create Subscription : ingester-downloader 2021/06/16 14:26:06 Create Subscription : ingester-processor 2021/06/16 14:26:06 Create Subscription : ingester-events 2021/06/16 14:26:06 Done! In order to run the ingester with the PubSub emulator, you must define the PUBSUB_EMULATOR_HOST environment variable (by default localhost:8085 ) before starting services. Downloader You can find the downloader main application in cmd/downloader folder. Build application: $ go build Downloader needs the path of the local install of ESA SNAP and the graph folder (at the root of geocube-ingester). Run application: $ export GRAPHPATH=<geocube-ingester>/graph $ export SNAPPATH=<path to ESA SNAP gpt binary> $ ./downloader -flag value Example: $ export GRAPHPATH=/home/user/geocube-ingester/graph $ export SNAPPATH=/usr/local/snap/bin/gpt $ export WORKING_DIR=/home/user/geocube-ingester/data/ $ ./downloader --ps-project geocube-emulator --job-queue ingester-downloader --event-queue ingester-events --local-path $WORKING_DIR/data --storage-uri $WORKING_DIR/output --workdir $WORKING_DIR/tmp -gs-provider-buckets=Sentinel2:gs://gcp-public-data-sentinel-2/tiles/{LATITUDE_BAND}/{GRID_SQUARE}/{GRANULE_ID}/{SCENE}.SAFE --copernicus-username=$COPERNICUS_USERNAME --copernicus-password=$COPERNICUS_PASSWORD For more information concerning flags and downloader argument, you can run: $ ./downloader --help Usage of ./downloader: -asf-token string ASF token (optional). To configure Alaska Satellite Facility as a potential image Provider. -creodias-password string creodias account password (optional) -creodias-username string creodias account username (optional). To configure Creodias as a potential image Provider. -docker-envs string docker variable env key white list (comma sep) -docker-mount-volumes string list of volumes to mount on the docker (comma separated) -docker-registry-password string password to authentication on private registry -docker-registry-server string address of server to authenticate on private registry -docker-registry-username string username to authentication on private registry (default \"_json_key\") -event-queue string name of the queue for job events (pgqueue or pubsub topic) -gs-provider-buckets string Google Storage buckets. List of constellation:bucket comma-separated (optional). To configure GS as a potential image Provider. bucket can contain several {IDENTIFIER} than will be replaced according to the sceneName. IDENTIFIER must be one of SCENE, MISSION_ID, PRODUCT_LEVEL, DATE(YEAR/MONTH/DAY), TIME(HOUR/MINUTE/SECOND), PDGS, ORBIT, TILE (LATITUDE_BAND/GRID_SQUARE/GRANULE_ID) -job-queue string name of the queue for downloader jobs (pgqueue or pubsub subscription) -local-path string local path where images are stored (optional). To configure a local path as a potential image Provider. -oneatlas-apikey string oneatlas apikey to use -oneatlas-auth-endpoint string oneatlas order endpoint to use (default \"https://authenticate.foundation.api.oneatlas.airbus.com/auth/realms/IDP/protocol/openid-connect/token\") -oneatlas-download-endpoint string oneatlas download endpoint to use (default \"https://access.foundation.api.oneatlas.airbus.com/api/v1/items\") -oneatlas-order-endpoint string oneatlas order endpoint to use (default \"https://data.api.oneatlas.airbus.com\") -oneatlas-username string oneatlas account username (optional). To configure Oneatlas as a potential image Provider. -peps-password string peps account password (optional) -peps-username string peps account username (optional). To configure PEPS as a potential image Provider. -pgq-connection string enable pgq messaging system with a connection to the database -ps-project string pubsub subscription project (gcp only/not required in local usage) -copernicus-password string copernicus account password (optional) -copernicus-username string copernicus account username (optional). To configure Copernicus as a potential image Provider. -storage-uri string storage uri (currently supported: local, gs). To store outputs of the scene preprocessing graph. -with-docker-engine activate the support of graph.engine == 'docker' (require a running docker-daemon) -workdir string working directory to store intermediate results (default \"/local-ssd\") Processor You can find the processor main application in cmd/processor folder. Build application: $ go build $ ls -l -rwxrwxr-x 1 user user 17063376 juin 11 15:45 processor -rw-rw-r-- 1 user user 11214 juin 9 16:09 main.go Processor needs the path of the local install of ESA SNAP and the graph folder (at the root of geocube-ingester). Run application: $ export GRAPHPATH=./graph $ export SNAPPATH=<path to ESA SNAP gpt binary> $ ./processor -flag value Example: $ export GRAPHPATH=./graph $ export SNAPPATH=/usr/local/snap/bin/gpt $ export WORKING_DIR=/home/user/geocube-ingester/data/ $ ./processor --ps-project geocube-emulator --job-queue ingester-processor --event-queue ingester-events --geocube-server $GEOCUBE_SERVER --geocube-insecure --storage-uri $WORKING_DIR/output --workdir $WORKING_DIR/tmp For more information concerning flags and downloader argument, you can run: $ ./processor --help Usage of ./processor: -docker-envs string docker variable env key white list (comma sep) -docker-mount-volumes string list of volumes to mount on the docker (comma separated) -docker-registry-password string password to authentication on private registry -docker-registry-server string address of server to authenticate on private registry -docker-registry-username string username to authentication on private registry (default \"_json_key\") -event-queue string name of the queue for job events (pgqueue or pubsub topic) -geocube-apikey string geocube server api key -geocube-insecure connection to geocube server is insecure -geocube-server string address of geocube server (default \"127.0.0.1:8080\") -job-queue string name of the queue for processor jobs (pgqueue or pubsub subscription) -pgq-connection string enable pgq messaging system with a connection to the database -ps-project string pubsub subscription project (gcp only/not required in local usage) -storage-uri string storage uri (currently supported: local, gs). To get outputs of the scene preprocessing graph and store outputs of the tile processing graph. -with-docker-engine activate the support of graph.engine == 'docker' (require a running docker-daemon) -workdir string working directory to store intermediate results (default \"/local-ssd\") Workflow You can find the workflow main application in cmd/workflow folder. Build application: $ go build $ ls -l -rwxrwxr-x 1 user user 17063376 juin 11 15:45 workflow -rw-rw-r-- 1 user user 11214 juin 9 16:09 main.go Run application: $ ./workflow -flag value Example: $ export DB_CONNECTION=postgresql://user:password@localhost:5432/ingester?binary_parameters=yes $ export WORKFLOW_PORT=8082 $ ./workflow --ps-project geocube-emulator --event-queue ingester-events --downloader-queue ingester-downloader --processor-queue ingester-processor --db-connection=$DB_CONNECTION --port $WORKFLOW_PORT --geocube-server $GEOCUBE_SERVER --geocube-insecure For more information concerning flags and downloader argument, you can run: $ ./workflow --help Usage of ./workflow: -bearer-auth string bearer authentication (token) (optional) -db-connection string database connection -downloader-queue string name of the queue for downloader jobs (pgqueue or pubsub topic) -downloader-rc string image-downloader replication controller name (autoscaler) -event-queue string name of the queue for job events (pgqueue or pubsub subscription) -gcstorage string GCS url where scenes are stored (for annotations) (optional) -geocube-apikey string geocube server api key -geocube-insecure connection to geocube server is insecure -geocube-server string address of geocube server (default \"127.0.0.1:8080\") -max-downloader int Max downloader instances (autoscaler) (default 10) -max-processor int Max Processor instances (autoscaler) (default 900) -namespace string namespace (autoscaler) -oneatlas-apikey string oneatlas account apikey (to generate an api key for your account: https://account.foundation.oneatlas.airbus.com/api-keys) -oneatlas-auth-endpoint string oneatlas order endpoint to use (default \"https://authenticate.foundation.api.oneatlas.airbus.com/auth/realms/IDP/protocol/openid-connect/token\") -oneatlas-endpoint string oneatlas endpoint to search products from the catalogue (default \"https://search.foundation.api.oneatlas.airbus.com/api/v2/opensearch\") -oneatlas-order-endpoint string oneatlas order endpoint to estimate processing price (default \"https://data.api.oneatlas.airbus.com\") -oneatlas-username string oneatlas account username (optional). To configure Oneatlas as a potential image Provider. -pgq-connection string enable pgq messaging system with a connection to the database -port string workflow port ot use (default \"8080\") -processor-queue string name of the queue for processor jobs (pgqueue or pubsub topic) -processor-rc string tile-processor replication controller name (autoscaler) -ps-project string pubsub subscription project (gcp only/not required in local usage) -tls enable TLS protocol (certificate and key must be /tls/tls.crt and /tls/tls.key)","title":"Local environment"},{"location":"installation/local-install/#local-installation","text":"","title":"Local installation"},{"location":"installation/local-install/#environment-of-development","text":"Name Version link Golang >= 1.23 https://golang.org/doc/install GDAL >= 3.8 https://gdal.org Python >= 3.7 https://www.python.org/downloads/ PostgreSQL >= 15 https://www.postgresql.org/download/ Docker NC https://docs.docker.com/engine/install/ (ESA SNAP) >=8.0 https://step.esa.int/main/download/snap-download/","title":"Environment of development"},{"location":"installation/local-install/#build-and-run-go-application","text":"","title":"Build and run Go application"},{"location":"installation/local-install/#messaging-broker","text":"","title":"Messaging broker"},{"location":"installation/local-install/#pgqueue","text":"To use this messaging broker, create the pgq_jobs table in your postgres database using the following script vendor/github.com/airbusgeo/geocube/interface/messaging/pgqueue/create_table.sql . $ psql -h <database_host> -d <database_name> -f vendor/github.com/airbusgeo/geocube/interface/messaging/pgqueue/create_table.sql Then, start the services the following arguments: - --pgq-connection : connection uri to the postgres database (e.g. postgresql://user:password@localhost:5432/geocube ) - --event-queue events - --downloader-queue/--job-queue downloader - --processor-queue/--job-queue processor","title":"PGQueue"},{"location":"installation/local-install/#pubsub-emulator","text":"A Pub/Sub emulator is available to use PubSub in a local system (with limited capacities). Please follow the documentation to start the emulator. Example: gcloud beta emulators pubsub start --project=geocube-emulator --host-port $PUBSUB_EMULATOR_HOST After starting pubsub emulator server, the following script creates ingester topics and subscriptions: tools/pubsub_emulator/main.go . $ go run tools/pubsub_emulator/main.go --project geocube-emulator 2021/06/16 14:26:06 New client for project geocube-emulator 2021/06/16 14:26:06 Create Topic : ingester-downloader 2021/06/16 14:26:06 Create Topic : ingester-processor 2021/06/16 14:26:06 Create Topic : ingester-events 2021/06/16 14:26:06 Create Subscription : ingester-downloader 2021/06/16 14:26:06 Create Subscription : ingester-processor 2021/06/16 14:26:06 Create Subscription : ingester-events 2021/06/16 14:26:06 Done! In order to run the ingester with the PubSub emulator, you must define the PUBSUB_EMULATOR_HOST environment variable (by default localhost:8085 ) before starting services.","title":"PubSub Emulator"},{"location":"installation/local-install/#downloader","text":"You can find the downloader main application in cmd/downloader folder. Build application: $ go build Downloader needs the path of the local install of ESA SNAP and the graph folder (at the root of geocube-ingester). Run application: $ export GRAPHPATH=<geocube-ingester>/graph $ export SNAPPATH=<path to ESA SNAP gpt binary> $ ./downloader -flag value Example: $ export GRAPHPATH=/home/user/geocube-ingester/graph $ export SNAPPATH=/usr/local/snap/bin/gpt $ export WORKING_DIR=/home/user/geocube-ingester/data/ $ ./downloader --ps-project geocube-emulator --job-queue ingester-downloader --event-queue ingester-events --local-path $WORKING_DIR/data --storage-uri $WORKING_DIR/output --workdir $WORKING_DIR/tmp -gs-provider-buckets=Sentinel2:gs://gcp-public-data-sentinel-2/tiles/{LATITUDE_BAND}/{GRID_SQUARE}/{GRANULE_ID}/{SCENE}.SAFE --copernicus-username=$COPERNICUS_USERNAME --copernicus-password=$COPERNICUS_PASSWORD For more information concerning flags and downloader argument, you can run: $ ./downloader --help Usage of ./downloader: -asf-token string ASF token (optional). To configure Alaska Satellite Facility as a potential image Provider. -creodias-password string creodias account password (optional) -creodias-username string creodias account username (optional). To configure Creodias as a potential image Provider. -docker-envs string docker variable env key white list (comma sep) -docker-mount-volumes string list of volumes to mount on the docker (comma separated) -docker-registry-password string password to authentication on private registry -docker-registry-server string address of server to authenticate on private registry -docker-registry-username string username to authentication on private registry (default \"_json_key\") -event-queue string name of the queue for job events (pgqueue or pubsub topic) -gs-provider-buckets string Google Storage buckets. List of constellation:bucket comma-separated (optional). To configure GS as a potential image Provider. bucket can contain several {IDENTIFIER} than will be replaced according to the sceneName. IDENTIFIER must be one of SCENE, MISSION_ID, PRODUCT_LEVEL, DATE(YEAR/MONTH/DAY), TIME(HOUR/MINUTE/SECOND), PDGS, ORBIT, TILE (LATITUDE_BAND/GRID_SQUARE/GRANULE_ID) -job-queue string name of the queue for downloader jobs (pgqueue or pubsub subscription) -local-path string local path where images are stored (optional). To configure a local path as a potential image Provider. -oneatlas-apikey string oneatlas apikey to use -oneatlas-auth-endpoint string oneatlas order endpoint to use (default \"https://authenticate.foundation.api.oneatlas.airbus.com/auth/realms/IDP/protocol/openid-connect/token\") -oneatlas-download-endpoint string oneatlas download endpoint to use (default \"https://access.foundation.api.oneatlas.airbus.com/api/v1/items\") -oneatlas-order-endpoint string oneatlas order endpoint to use (default \"https://data.api.oneatlas.airbus.com\") -oneatlas-username string oneatlas account username (optional). To configure Oneatlas as a potential image Provider. -peps-password string peps account password (optional) -peps-username string peps account username (optional). To configure PEPS as a potential image Provider. -pgq-connection string enable pgq messaging system with a connection to the database -ps-project string pubsub subscription project (gcp only/not required in local usage) -copernicus-password string copernicus account password (optional) -copernicus-username string copernicus account username (optional). To configure Copernicus as a potential image Provider. -storage-uri string storage uri (currently supported: local, gs). To store outputs of the scene preprocessing graph. -with-docker-engine activate the support of graph.engine == 'docker' (require a running docker-daemon) -workdir string working directory to store intermediate results (default \"/local-ssd\")","title":"Downloader"},{"location":"installation/local-install/#processor","text":"You can find the processor main application in cmd/processor folder. Build application: $ go build $ ls -l -rwxrwxr-x 1 user user 17063376 juin 11 15:45 processor -rw-rw-r-- 1 user user 11214 juin 9 16:09 main.go Processor needs the path of the local install of ESA SNAP and the graph folder (at the root of geocube-ingester). Run application: $ export GRAPHPATH=./graph $ export SNAPPATH=<path to ESA SNAP gpt binary> $ ./processor -flag value Example: $ export GRAPHPATH=./graph $ export SNAPPATH=/usr/local/snap/bin/gpt $ export WORKING_DIR=/home/user/geocube-ingester/data/ $ ./processor --ps-project geocube-emulator --job-queue ingester-processor --event-queue ingester-events --geocube-server $GEOCUBE_SERVER --geocube-insecure --storage-uri $WORKING_DIR/output --workdir $WORKING_DIR/tmp For more information concerning flags and downloader argument, you can run: $ ./processor --help Usage of ./processor: -docker-envs string docker variable env key white list (comma sep) -docker-mount-volumes string list of volumes to mount on the docker (comma separated) -docker-registry-password string password to authentication on private registry -docker-registry-server string address of server to authenticate on private registry -docker-registry-username string username to authentication on private registry (default \"_json_key\") -event-queue string name of the queue for job events (pgqueue or pubsub topic) -geocube-apikey string geocube server api key -geocube-insecure connection to geocube server is insecure -geocube-server string address of geocube server (default \"127.0.0.1:8080\") -job-queue string name of the queue for processor jobs (pgqueue or pubsub subscription) -pgq-connection string enable pgq messaging system with a connection to the database -ps-project string pubsub subscription project (gcp only/not required in local usage) -storage-uri string storage uri (currently supported: local, gs). To get outputs of the scene preprocessing graph and store outputs of the tile processing graph. -with-docker-engine activate the support of graph.engine == 'docker' (require a running docker-daemon) -workdir string working directory to store intermediate results (default \"/local-ssd\")","title":"Processor"},{"location":"installation/local-install/#workflow","text":"You can find the workflow main application in cmd/workflow folder. Build application: $ go build $ ls -l -rwxrwxr-x 1 user user 17063376 juin 11 15:45 workflow -rw-rw-r-- 1 user user 11214 juin 9 16:09 main.go Run application: $ ./workflow -flag value Example: $ export DB_CONNECTION=postgresql://user:password@localhost:5432/ingester?binary_parameters=yes $ export WORKFLOW_PORT=8082 $ ./workflow --ps-project geocube-emulator --event-queue ingester-events --downloader-queue ingester-downloader --processor-queue ingester-processor --db-connection=$DB_CONNECTION --port $WORKFLOW_PORT --geocube-server $GEOCUBE_SERVER --geocube-insecure For more information concerning flags and downloader argument, you can run: $ ./workflow --help Usage of ./workflow: -bearer-auth string bearer authentication (token) (optional) -db-connection string database connection -downloader-queue string name of the queue for downloader jobs (pgqueue or pubsub topic) -downloader-rc string image-downloader replication controller name (autoscaler) -event-queue string name of the queue for job events (pgqueue or pubsub subscription) -gcstorage string GCS url where scenes are stored (for annotations) (optional) -geocube-apikey string geocube server api key -geocube-insecure connection to geocube server is insecure -geocube-server string address of geocube server (default \"127.0.0.1:8080\") -max-downloader int Max downloader instances (autoscaler) (default 10) -max-processor int Max Processor instances (autoscaler) (default 900) -namespace string namespace (autoscaler) -oneatlas-apikey string oneatlas account apikey (to generate an api key for your account: https://account.foundation.oneatlas.airbus.com/api-keys) -oneatlas-auth-endpoint string oneatlas order endpoint to use (default \"https://authenticate.foundation.api.oneatlas.airbus.com/auth/realms/IDP/protocol/openid-connect/token\") -oneatlas-endpoint string oneatlas endpoint to search products from the catalogue (default \"https://search.foundation.api.oneatlas.airbus.com/api/v2/opensearch\") -oneatlas-order-endpoint string oneatlas order endpoint to estimate processing price (default \"https://data.api.oneatlas.airbus.com\") -oneatlas-username string oneatlas account username (optional). To configure Oneatlas as a potential image Provider. -pgq-connection string enable pgq messaging system with a connection to the database -port string workflow port ot use (default \"8080\") -processor-queue string name of the queue for processor jobs (pgqueue or pubsub topic) -processor-rc string tile-processor replication controller name (autoscaler) -ps-project string pubsub subscription project (gcp only/not required in local usage) -tls enable TLS protocol (certificate and key must be /tls/tls.crt and /tls/tls.key)","title":"Workflow"},{"location":"installation/others/","text":"Other environments Geocube-ingester is designed to be customizable and deployable in other environments. But it may be necessary to implemente new interfaces. More information on interfaces here","title":"Customize environment"},{"location":"installation/others/#other-environments","text":"Geocube-ingester is designed to be customizable and deployable in other environments. But it may be necessary to implemente new interfaces. More information on interfaces here","title":"Other environments"},{"location":"installation/prerequisite/","text":"Prerequisites The ingester needs: a relational database (currently supported Postgresql >= 11) a messaging System to exchange messages between services (currently supported: Pub/Sub, PGQueue: based on postgresql) an Object Storage, readable and writable (currently supported: local storage or GCS) a catalogue service, to get the list of products available over an AOI ( currently supported ) an image provider, to download the products returned by the catalogue service ( currently supported )","title":"Prerequisite"},{"location":"installation/prerequisite/#prerequisites","text":"The ingester needs: a relational database (currently supported Postgresql >= 11) a messaging System to exchange messages between services (currently supported: Pub/Sub, PGQueue: based on postgresql) an Object Storage, readable and writable (currently supported: local storage or GCS) a catalogue service, to get the list of products available over an AOI ( currently supported ) an image provider, to download the products returned by the catalogue service ( currently supported )","title":"Prerequisites"},{"location":"user-guide/catalog/","text":"Catalogue NB: This documentation is for user that want to use the Catalogue. For documentation on how to implement a new catalogue, see Developer-Guide/Catalogue . The Catalogue component makes an inventory of all the scenes (and bursts for S1 images) covering the AOI between startDate and endDate, depending on user-defined criteria. The results, including the product metadata, are formated in a standard way, following the GeoJSON standard and ready to be ingested. See payload to create a payload. List of implemented catalogues : Copernicus : sentinel1 & 2 scenes Creodias : sentinel1 & 2 scenes Landsat AWS : Landsat 8 & 9 OneAtlas : PHR & SPOT scenes GCS or AWS : to retrieve the Sentinel-1 annotations Constellations Sentinel constellations Copernicus Supported constellations: sentinel1 sentinel2 Copernicus can be used to list the Sentinel products. It does not require authentication. Use the --copernicus-catalog flag to enable this catalogue. For more information see: Copernicus OpenSearch API Documentation Copernicus ODATA API Documentation Creodias Supported constellations: sentinel1 sentinel2 No authentication required. Use the --creodias-catalog flag to enable this catalogue. NB: Creodias is usually more reliable than Copernicus, but Sentinel-1 catalogue returns less information than the Copernicus' one. For more information see: Creodias API USGS constellations Landsat AWS Supported constellations: landsat89 It uses the STAC interface to list the Landsat products available on AWS. No authentication required. Use the --landsat-aws-catalog flag to enable this catalogue. For more information see: USGS Landsat Airbus constellations OneAtlas Supported constellations: spot pleiades / phr Use the following arguments to configure this catalogue: - oneatlas-username - oneatlas-apikey - oneatlas-endpoint - oneatlas-order-endpoint - oneatlas-auth-endpoint Account In order to use oneAtlas, you need to create an account here . But if you would like to give our service a try before purchasing, you can sign up for a 1 month Free Trial by signing up on our website here . Concerning authentication, you need to create an APIKEY here (more documentation is available here ) Shortly after you can learn about managing your account and subscriptions through our Manage Contract Guide . Once your account is created you should be ready to search! Take a look at our image catalog, the \u2018Living Library\u2019. High resolution images are added continuously on a daily basis. It is designed to offer an extensive set of search criteria which you can find in our Search Guide . Imagery OneAtlas Catalog is requested in order to download PHR, SPOT Products in Dimap format. Catalog provides an estimated cost of a potential processing order (available in ScenesInventory) Sentinel-1 bursts annotations To list the bursts of a Sentinel-1 product without downloading the file, the catalogue has to download the annotation file included in the .SAFE file. Object storage Local storage, GCS or AWS can be used to retrieve burst annotations from archives (.SAFE.zip) stored in a user bucket. User account must have the appropriate rights to access the bucket ( -annotations-urls ). Outputs It returns a list of Scenes with associated Tiles, ready to be ingested. For each scene, metadata provided by the catalogue are added to the tags of the record definining the scene: - `sourceID`: Name of the product - `uuid`: Universally unique identifier in the catalogue (if exists) - `productType`: Type of the product, containing the sensor and the level of processing (e.g. `S2MSIL1C` for Sentinel2, `LC_C2_L1TP` for Landsat) - `ingestionDate`: Date-time of acquisition - `constellation` - `satellite` - `orbit`: Absolute orbits is the number of orbits taken from the beginning of the mission. - `orbitDirection` - `relativeOrbit`: Relative orbit tells you where you are in the repeating orbit-cycle. - `downloadURL` (if provided) - `sunAzimuth`, `sunElevation` - `incidenceAngle`, `incidenceAzimuth` For optical products: - `cloudCoverPercentage` - `landCloudCoverPercentage`: only clouds covering land For SAR products: - `polarisationMode` - `sliceNumber` (Sentinel1) - `lastRelativeOrbit` (Sentinel1) - `lastOrbit` (Sentinel1)","title":"Catalogue service"},{"location":"user-guide/catalog/#catalogue","text":"NB: This documentation is for user that want to use the Catalogue. For documentation on how to implement a new catalogue, see Developer-Guide/Catalogue . The Catalogue component makes an inventory of all the scenes (and bursts for S1 images) covering the AOI between startDate and endDate, depending on user-defined criteria. The results, including the product metadata, are formated in a standard way, following the GeoJSON standard and ready to be ingested. See payload to create a payload. List of implemented catalogues : Copernicus : sentinel1 & 2 scenes Creodias : sentinel1 & 2 scenes Landsat AWS : Landsat 8 & 9 OneAtlas : PHR & SPOT scenes GCS or AWS : to retrieve the Sentinel-1 annotations","title":"Catalogue"},{"location":"user-guide/catalog/#constellations","text":"","title":"Constellations"},{"location":"user-guide/catalog/#sentinel-constellations","text":"","title":"Sentinel constellations"},{"location":"user-guide/catalog/#copernicus","text":"Supported constellations: sentinel1 sentinel2 Copernicus can be used to list the Sentinel products. It does not require authentication. Use the --copernicus-catalog flag to enable this catalogue. For more information see: Copernicus OpenSearch API Documentation Copernicus ODATA API Documentation","title":"Copernicus"},{"location":"user-guide/catalog/#creodias","text":"Supported constellations: sentinel1 sentinel2 No authentication required. Use the --creodias-catalog flag to enable this catalogue. NB: Creodias is usually more reliable than Copernicus, but Sentinel-1 catalogue returns less information than the Copernicus' one. For more information see: Creodias API","title":"Creodias"},{"location":"user-guide/catalog/#usgs-constellations","text":"","title":"USGS constellations"},{"location":"user-guide/catalog/#landsat-aws","text":"Supported constellations: landsat89 It uses the STAC interface to list the Landsat products available on AWS. No authentication required. Use the --landsat-aws-catalog flag to enable this catalogue. For more information see: USGS Landsat","title":"Landsat AWS"},{"location":"user-guide/catalog/#airbus-constellations","text":"","title":"Airbus constellations"},{"location":"user-guide/catalog/#oneatlas","text":"Supported constellations: spot pleiades / phr Use the following arguments to configure this catalogue: - oneatlas-username - oneatlas-apikey - oneatlas-endpoint - oneatlas-order-endpoint - oneatlas-auth-endpoint","title":"OneAtlas"},{"location":"user-guide/catalog/#account","text":"In order to use oneAtlas, you need to create an account here . But if you would like to give our service a try before purchasing, you can sign up for a 1 month Free Trial by signing up on our website here . Concerning authentication, you need to create an APIKEY here (more documentation is available here ) Shortly after you can learn about managing your account and subscriptions through our Manage Contract Guide . Once your account is created you should be ready to search! Take a look at our image catalog, the \u2018Living Library\u2019. High resolution images are added continuously on a daily basis. It is designed to offer an extensive set of search criteria which you can find in our Search Guide .","title":"Account"},{"location":"user-guide/catalog/#imagery","text":"OneAtlas Catalog is requested in order to download PHR, SPOT Products in Dimap format. Catalog provides an estimated cost of a potential processing order (available in ScenesInventory)","title":"Imagery"},{"location":"user-guide/catalog/#sentinel-1-bursts-annotations","text":"To list the bursts of a Sentinel-1 product without downloading the file, the catalogue has to download the annotation file included in the .SAFE file.","title":"Sentinel-1 bursts annotations"},{"location":"user-guide/catalog/#object-storage","text":"Local storage, GCS or AWS can be used to retrieve burst annotations from archives (.SAFE.zip) stored in a user bucket. User account must have the appropriate rights to access the bucket ( -annotations-urls ).","title":"Object storage"},{"location":"user-guide/catalog/#outputs","text":"It returns a list of Scenes with associated Tiles, ready to be ingested. For each scene, metadata provided by the catalogue are added to the tags of the record definining the scene: - `sourceID`: Name of the product - `uuid`: Universally unique identifier in the catalogue (if exists) - `productType`: Type of the product, containing the sensor and the level of processing (e.g. `S2MSIL1C` for Sentinel2, `LC_C2_L1TP` for Landsat) - `ingestionDate`: Date-time of acquisition - `constellation` - `satellite` - `orbit`: Absolute orbits is the number of orbits taken from the beginning of the mission. - `orbitDirection` - `relativeOrbit`: Relative orbit tells you where you are in the repeating orbit-cycle. - `downloadURL` (if provided) - `sunAzimuth`, `sunElevation` - `incidenceAngle`, `incidenceAzimuth` For optical products: - `cloudCoverPercentage` - `landCloudCoverPercentage`: only clouds covering land For SAR products: - `polarisationMode` - `sliceNumber` (Sentinel1) - `lastRelativeOrbit` (Sentinel1) - `lastOrbit` (Sentinel1)","title":"Outputs"},{"location":"user-guide/graph/","text":"Graph Principle A processing graph defines a sequence of steps which will be executed on the input images. Each step is a command with parameters and configurations . A logical condition can be added to the execution of the step. Each step is executed with an engine among Python , Snap , cmd and Docker . Graph should have input or/and output files (depends on kind of processing). JSON Graph Structure type ProcessingGraphJSON struct { Config map[string]string `json:\"config\"` Envs []string `json:\"envs,omitempty\"` InFiles [3][]InFile `json:\"in_files\"` OutFiles [][]OutFile `json:\"out_files\"` Steps []ProcessingStep `json:\"processing_steps\"` } Config List of configuration values that can be used as parameters of a Command , using ArgConfig structure (see Args ). Files A file is defined by its name ( Layer field) and its Extension (it must be consistent between the scene graph and the tile graph ). Examples of Layer name const ( Product Layer = \"__product__\" // Special value, that will be replaced by the SourceID of the Scene Annotations Layer = \"__annotations__\" // Special value, that will be replaced by the SourceID of the Scene + \"_annotations\" suffix LayerPreprocessed Layer = \"preprocessed\" LayerCoregistrated Layer = \"coregistred\" LayerCoregExtract Layer = \"coregextract\" LayerCoherence Layer = \"coherence\" LayerBackscatterVV Layer = \"sigma0_VV\" LayerBackscatterVH Layer = \"sigma0_VH\" LayerCoherenceVV Layer = \"coh_VV\" LayerCoherenceVH Layer = \"coh_VH\" LayerPanchromatic Layer = \"P\" LayerMultiSpectral Layer = \"MS\" ) Examples of File Extension const ( NoExtension Extension = \"\" // The layer has no extension ExtensionGTiff Extension = \"tif\" ExtensionZIP Extension = \"zip\" // The following extensions are directories, thus, they are stored as a zip file (see service.storeAsZip() function) // Using those extensions ensures that the stored files will be unzipped in a directory named <layer>.<Extension> ExtensionSAFE Extension = \"SAFE\" // Sentinel product ExtensionDIMAP Extension = \"dim\" ExtensionDIMAPData Extension = \"data\" ExtensionAll Extension = \"*\" // The content of the whole working directory (e.g. useful to export all the downloaded files as one zip file). Replaced by NoExtension in the directory name ) Input Files For the processor graph file only InFile defines a file used in input of the processing. It can be either a product that has just be downloaded by the downloader or a list of Layer that will be processed by the processor . The files defined in InFile are automatically downloaded from the storage and stored locally during the processing. Structure // InFile describes an input file of the processing type InFile struct { File Condition TileCondition `json:\"condition\"` } Example To use the whole product as input { \"in_files\":[ [ { \"layer\":\"__product__\", \"extension\":\"SAFE\" } ], [], [] ] } To use a specific layer that has been preprocessed by the downloader: { \"in_files\":[ [ { \"layer\":\"coregistrated\", \"extension\":\"tif\" } ], [], [] ] } Infiles json block is an array of 3. The first value is always relative to the current product. The second and third values can be used to reference other products, such as the previous in the timeserie or the first of the timeserie (e.g: to have a unique reference for all the images in the timeserie). For instance, it is useful in order to process coherence cf. Example with S1 Output Files OutFile defines a file (name ( Layer ) and extension) generated by the processing steps. Output files can be stored in the storage, indexed in the geocube (for processor only), or flagged as to be deleted. If an OutFile is to be indexed in the Geocube, the following information must be provided (for other action, they will be ignored): - DType - NoData - Min , Max - ExtMin , ExtMax - Exponent - Nbands See geocube indexation documentation for further information. Structure // OutFile describes an output file of the processing type OutFile struct { File dformatOut Arg // argFixed or argConfig DType DType `json:\"datatype\"` NoData float64 `json:\"nodata\"` Min float64 `json:\"min_value\"` Max float64 `json:\"max_value\"` ExtMin float64 `json:\"ext_min_value\"` ExtMax float64 `json:\"ext_max_value\"` Exponent float64 `json:\"exponent\"` // JSON default: 1 Nbands int `json:\"nbands\"` // JSON default: 1 Action OutFileAction `json:\"action\"` Condition TileCondition `json:\"condition\"` // JSON default: pass } Available action // OutFileAction const ( ToIgnore OutFileAction = iota `json:\"to_ignore\"` ToCreate `json:\"to_create\"` ToIndex `json:\"to_index\"` ToDelete `json:\"to_delete\"` ) Example { \"out_files\":[ [ { \"layer\":\"img\", \"extension\":\"tif\", \"action\":\"to_index\", \"dformat_out\":{ \"type\":\"fixed\", \"value\":\"uint16,0,0,30000\" }, \"ext_min_value\":0, \"ext_max_value\":3, \"nbands\":4 }, { \"layer\":\"clcsh\", \"extension\":\"tif\", \"action\":\"to_create\", }, { \"layer\":\"__product__\", \"extension\":\"SAFE\", \"action\":\"to_delete\", \"error_condition\": \"on_fatal_failure\" } ], [], [] ] } Steps List of processing steps that will be executed sequentially. Engine Define the kind of engine that will be used to execute the step. Available engines: Snap: for Sentinel constellation (if installed) Python: to execute python script Docker: to run a docker Cmd: for command available in the docker Command Define the command to execute with the given engine. MyBinary Example: { \"processing_steps\": [ { \"engine\": \"cmd\", \"command\": \"./cmd/MyBinary\" } ] } Docker Example: { \"engine\": \"docker\", \"command\": \"containerRegistry/myImage:myTag\" } Args List of arguments that will be passed to the command, with the following synthax : --argument-name argument-value . They can be of five types: ArgIn : The path of the file corresponding to a layer in InFiles . ArgOut : The path of the file corresponding to a layer in OutFiles . ArgFixed : A fixed value ArgConfig : A value retrieved from the Config using a key. The key workdir is defined at runtime to point to the working directory. ArgTile : A value retrieved from the tile. Supported value: constellation scene name date number : tile number (for Sentinel-1: =burst number) swath (Sentinel-1) cohdate : (Sentinel-1) Date of the reference burst if different from previous date or date of the burst Structure: type ArgIn struct { Input int `json:\"tile_index\"` // Index of input [0, 1, 2] Layer service.Layer `json:\"layer\"` Extension service.Extension `json:\"extension\"` } type ArgOut struct { service.Layer `json:\"layer\"` Extension service.Extension `json:\"extension\"` } type ArgFixed string // fixed arg type ArgConfig string // arg from config type ArgTile string // arg from tile info Example { \"engine\":\"docker\", \"command\":\"containerRegistry/myImage:myTag\", \"args\":{ \"workdir\":{ \"type\":\"config\", \"value\":\"workdir\" }, \"image-in\":{ \"type\":\"in\", \"layer\":\"__product__\", \"extension\":\"SAFE\" }, \"parameter-from-config\":{ \"type\":\"config\", \"value\":\"key-in-config\" }, \"constellation\":{ \"type\":\"fixed\", \"value\":\"sentinel2\" }, \"out-file\":{ \"type\":\"out\", \"layer\":\"img\", \"extension\":\"tif\" }, \"out-pattern\":{ \"type\":\"out\", \"layer\":\"*\", \"extension\":\"tif\" } } } This step will run something similar to: docker run -v <volume>:<workdir> containerRegistry/myImage:myTag --workdir <config['workdir']> --image-in <path to in_file[0]/product.SAFE> --parameter-from-config <config['key-in-config']> --constellation sentinel2 --out-file <path to out_file[0]/img.tif> --out-pattern <path to out_file[0]>/*.tif Condition Condition to optionally execute the step. Structure type TileCondition struct { Name string Pass func([]common.Tile) bool } Available tileConditions // condPass is a tileCondition always true var pass = TileCondition{\"pass\", func(tiles []common.Tile) bool { return true }} // condDiffTile returns true if tile1 != tile2 var condDiffT0T1 = TileCondition{\"different_T0_T1\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID != tiles[1].Scene.SourceID }} var condDiffT0T2 = TileCondition{\"different_T0_T2\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID != tiles[2].Scene.SourceID }} var condDiffT1T2 = TileCondition{\"different_T1_T2\", func(tiles []common.Tile) bool { return tiles[1].Scene.SourceID != tiles[2].Scene.SourceID }} // condEqualTile returns true if tile1 == tile2 var condEqualT0T1 = TileCondition{\"equal_T0_T1\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID == tiles[1].Scene.SourceID }} var condEqualT0T2 = TileCondition{\"equal_T0_T2\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID == tiles[2].Scene.SourceID }} var condEqualT1T2 = TileCondition{\"equal_T1_T2\", func(tiles []common.Tile) bool { return tiles[1].Scene.SourceID == tiles[2].Scene.SourceID }} Example { \"condition\": \"pass\" } Example Processing Sentinel1 { \"config\": { \"bs_erode_iterations\": \"10\", \"coh_erode_iterations\": \"10\", \"coherence_azimuth\": \"4\", \"coherence_range\": \"16\", \"dem_egm_correction\": \"True\", \"dem_file\": \"\", \"dem_name\": \"SRTM 3Sec\", \"dem_nodata\": \"0\", \"dem_resampling\": \"BILINEAR_INTERPOLATION\", \"dformat_out\": \"float32,0,0,1\", \"img_resampling\": \"BICUBIC_INTERPOLATION\", \"bkg_resampling\": \"BISINC_21_POINT_INTERPOLATION\", \"projection\": \"EPSG:4326\", \"resolution\": \"20\", \"snap_cpu_parallelism\": \"1\", \"terrain_correction_azimuth\": \"1\", \"terrain_correction_range\": \"4\" }, \"processing_steps\": [ { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_BkG.xml\", \"args\": { \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"resampling\": { \"type\": \"config\", \"value\": \"bkg_resampling\" }, \"master\": { \"type\": \"in\", \"tile_index\": 2, \"layer\": \"preprocessed\", \"extension\": \"dim\" }, \"output\": { \"type\": \"out\", \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"slave\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"preprocessed\", \"extension\": \"dim\" } }, \"condition\": \"pass\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_SlvExtract.xml\", \"args\": { \"input\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"output\": { \"type\": \"out\", \"layer\": \"coregextract\", \"extension\": \"dim\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_Deb_BetaSigma_ML_TC_RNKELL.xml\", \"args\": { \"azimuth_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_azimuth\" }, \"band\": { \"type\": \"fixed\", \"value\": \"Sigma0\" }, \"dem_egm\": { \"type\": \"config\", \"value\": \"dem_egm_correction\" }, \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"grid_align\": { \"type\": \"fixed\", \"value\": \"true\" }, \"img_resampling\": { \"type\": \"config\", \"value\": \"img_resampling\" }, \"img_suffix\": { \"type\": \"tile\", \"value\": \"date\" }, \"input\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"outputVH\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"outputVV\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"projection\": { \"type\": \"config\", \"value\": \"projection\" }, \"range_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_range\" }, \"resolution\": { \"type\": \"config\", \"value\": \"resolution\" }, \"swath\": { \"type\": \"tile\", \"value\": \"swath\" }, \"trig\": { \"type\": \"fixed\", \"value\": \"sin\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"bs_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"bs_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"pass\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_BkG.xml\", \"args\": { \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"resampling\": { \"type\": \"config\", \"value\": \"bkg_resampling\" }, \"master\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregextract\", \"extension\": \"dim\" }, \"output\": { \"type\": \"out\", \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"slave\": { \"type\": \"in\", \"tile_index\": 1, \"layer\": \"coregextract\", \"extension\": \"dim\" } }, \"condition\": \"different_T1_T2\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_Coh_BSel_Deb_ML_TC.xml\", \"args\": { \"azimuth_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_azimuth\" }, \"coherence_azimuth\": { \"type\": \"config\", \"value\": \"coherence_azimuth\" }, \"coherence_range\": { \"type\": \"config\", \"value\": \"coherence_range\" }, \"dem_egm\": { \"type\": \"config\", \"value\": \"dem_egm_correction\" }, \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"grid_align\": { \"type\": \"fixed\", \"value\": \"true\" }, \"img_resampling\": { \"type\": \"config\", \"value\": \"img_resampling\" }, \"input\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"outputVH\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"outputVV\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"projection\": { \"type\": \"config\", \"value\": \"projection\" }, \"range_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_range\" }, \"resolution\": { \"type\": \"config\", \"value\": \"resolution\" }, \"sel_date\": { \"type\": \"tile\", \"value\": \"cohdate\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"coh_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"coh_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"different_T0_T1\" } ], \"in_files\": [ [ { \"layer\": \"preprocessed\", \"extension\": \"dim\" } ], [ { \"layer\": \"coregextract\", \"extension\": \"dim\", \"condition\": \"different_T1_T2\" } ], [ { \"layer\": \"preprocessed\", \"extension\": \"dim\", \"condition\": \"different_T0_T2\" } ] ], \"out_files\": [ [ { \"layer\": \"sigma0_VV\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\" }, { \"layer\": \"sigma0_VH\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\" }, { \"layer\": \"coh_VV\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\", \"condition\": \"different_T0_T1\" }, { \"layer\": \"coh_VH\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\", \"condition\": \"different_T0_T1\" } ], [ ], [] ] } Example Processing Sentinel2 { \"config\": { \"srtm-uri\": \"...\" }, \"processing_steps\": [ { \"engine\": \"cmd\", \"command\": \"./cmd/radiometric_processor\", \"args\": { \"workdir\": { \"type\": \"config\", \"value\": \"workdir\" }, \"image\": { \"type\": \"in\", \"layer\": \"__product__\", \"extension\": \"SAFE\" }, \"srtm-uri\": { \"type\": \"config\", \"value\": \"srtm-uri\" }, \"constellation\": { \"type\": \"fixed\", \"value\": \"sentinel2\" }, \"out-pattern\": { \"type\": \"out\", \"layer\": \"*\", \"extension\": \"tif\" } }, \"condition\": \"pass\" }, { \"engine\": \"cmd\", \"command\": \"./cmd/registation\", \"args\": { \"workdir\": { \"type\": \"config\", \"value\": \"workdir\" }, \"satellite-image\": { \"type\": \"in\", \"layer\": \"__product__\", \"extension\": \"SAFE\" }, \"input\": { \"type\": \"fixed\", \"value\": \"*.tif\"}, \"output\": { \"type\": \"fixed\", \"value\": \".\" }, \"constellation\": { \"type\": \"fixed\", \"value\": \"Sentinel2\" } }, \"condition\": \"pass\" } ], \"in_files\": [ [ { \"layer\": \"__product__\", \"extension\": \"SAFE\" } ], [], [] ], \"out_files\": [ [ { \"layer\": \"img\", \"extension\": \"tif\", \"action\": \"to_index\", \"dformat_out\": { \"type\": \"fixed\", \"value\": \"uint16,0,0,30000\" }, \"ext_min_value\": 0, \"ext_max_value\": 3, \"nbands\": 4 }, { \"layer\": \"quality_mask\", \"extension\": \"tif\", \"action\": \"to_index\", \"dformat_out\": { \"type\": \"fixed\", \"value\": \"int16,-1,0,10000\" }, \"ext_min_value\": 0, \"ext_max_value\": 1 } ], [], [] ] } Docker Engine Principle & Concept Instead of using and runing one large image docker which contains every dependencies (Snap, Python, other binaries), the ingester processor can be a docker container orchestrator. Processor Replication controller can be configured to run two containers: Generic container with processor Go binary and docker binary (Client) Dind (docker in docker) container which contains docker daemon (Server) Deployment In order to deploy this kind of architecture, some configuration must be added in your k8s config file: In client container (environment var): - name: DOCKER_HOST value: tcp://localhost:2375 A certificate can be defined in order to secure your server or enable insecure trafic. In Server container (environment var): - name: DOCKER_TLS_CERTDIR value: \"\" Private Registry In order to use private registry, credentials must be configured in Client side container. user (for gcp registry: _json_key ) password (for gcp registry: service account content) server (for gcp registry e.g: https://europe-west1-docker.pkg.dev ) For images stored in public registry (ex. docker.hub), there is no need for credentials. Credentials Some containers need file credentials (to request storage for example). docker-mount-volumes can be defined in client executable arguments to mount a volume containing credential file. Variable environment must be also defined in graph file: { \"engine\": \"docker\", \"command\": \"myImage\", \"args\": { \"workdir\": { \"type\": \"config\", \"value\": \"workdir\" }, \"image\": { \"type\": \"in\", \"layer\": \"__product__\", \"extension\": \"SAFE\" }, \"srtm-uri\": { \"type\": \"config\", \"value\": \"srtm-uri\" }, \"roadmap\": { \"type\": \"config\", \"value\": \"roadmap-robot\" }, \"options\": { \"type\": \"config\", \"value\": \"roadmap-options\" }, \"roadmap-params\": { \"type\": \"config\", \"value\": \"roadmap-params\" }, \"libs\": { \"type\": \"config\", \"value\": \"libs\" }, \"constellation\": { \"type\": \"fixed\", \"value\": \"sentinel2\" }, \"out-pattern\": { \"type\": \"out\", \"layer\": \"*\", \"extension\": \"tif\" } }, \"envs\": [\"GOOGLE_APPLICATION_CREDENTIALS=/myPath/myFile\"], \"condition\": \"pass\" } Important: Path defined in args and path in graphFile should be equal. Volume In order to mount volume correctly, it is important to enable access to all containers (processor & docker-daemon) Example: If you want to use a workdir which is mount on /local-sdd in your processor kubernetes deployment. If you want to mount also this workdir on your docker processing, you need to mount this volume on processor container and docker-dind container to enable docker-daemon access.","title":"Processing graphs"},{"location":"user-guide/graph/#graph","text":"","title":"Graph"},{"location":"user-guide/graph/#principle","text":"A processing graph defines a sequence of steps which will be executed on the input images. Each step is a command with parameters and configurations . A logical condition can be added to the execution of the step. Each step is executed with an engine among Python , Snap , cmd and Docker . Graph should have input or/and output files (depends on kind of processing).","title":"Principle"},{"location":"user-guide/graph/#json-graph","text":"","title":"JSON Graph"},{"location":"user-guide/graph/#structure","text":"type ProcessingGraphJSON struct { Config map[string]string `json:\"config\"` Envs []string `json:\"envs,omitempty\"` InFiles [3][]InFile `json:\"in_files\"` OutFiles [][]OutFile `json:\"out_files\"` Steps []ProcessingStep `json:\"processing_steps\"` }","title":"Structure"},{"location":"user-guide/graph/#config","text":"List of configuration values that can be used as parameters of a Command , using ArgConfig structure (see Args ).","title":"Config"},{"location":"user-guide/graph/#files","text":"A file is defined by its name ( Layer field) and its Extension (it must be consistent between the scene graph and the tile graph ).","title":"Files"},{"location":"user-guide/graph/#examples-of-layer-name","text":"const ( Product Layer = \"__product__\" // Special value, that will be replaced by the SourceID of the Scene Annotations Layer = \"__annotations__\" // Special value, that will be replaced by the SourceID of the Scene + \"_annotations\" suffix LayerPreprocessed Layer = \"preprocessed\" LayerCoregistrated Layer = \"coregistred\" LayerCoregExtract Layer = \"coregextract\" LayerCoherence Layer = \"coherence\" LayerBackscatterVV Layer = \"sigma0_VV\" LayerBackscatterVH Layer = \"sigma0_VH\" LayerCoherenceVV Layer = \"coh_VV\" LayerCoherenceVH Layer = \"coh_VH\" LayerPanchromatic Layer = \"P\" LayerMultiSpectral Layer = \"MS\" )","title":"Examples of Layer name"},{"location":"user-guide/graph/#examples-of-file-extension","text":"const ( NoExtension Extension = \"\" // The layer has no extension ExtensionGTiff Extension = \"tif\" ExtensionZIP Extension = \"zip\" // The following extensions are directories, thus, they are stored as a zip file (see service.storeAsZip() function) // Using those extensions ensures that the stored files will be unzipped in a directory named <layer>.<Extension> ExtensionSAFE Extension = \"SAFE\" // Sentinel product ExtensionDIMAP Extension = \"dim\" ExtensionDIMAPData Extension = \"data\" ExtensionAll Extension = \"*\" // The content of the whole working directory (e.g. useful to export all the downloaded files as one zip file). Replaced by NoExtension in the directory name )","title":"Examples of File Extension"},{"location":"user-guide/graph/#input-files","text":"For the processor graph file only InFile defines a file used in input of the processing. It can be either a product that has just be downloaded by the downloader or a list of Layer that will be processed by the processor . The files defined in InFile are automatically downloaded from the storage and stored locally during the processing.","title":"Input Files"},{"location":"user-guide/graph/#structure_1","text":"// InFile describes an input file of the processing type InFile struct { File Condition TileCondition `json:\"condition\"` }","title":"Structure"},{"location":"user-guide/graph/#example","text":"To use the whole product as input { \"in_files\":[ [ { \"layer\":\"__product__\", \"extension\":\"SAFE\" } ], [], [] ] } To use a specific layer that has been preprocessed by the downloader: { \"in_files\":[ [ { \"layer\":\"coregistrated\", \"extension\":\"tif\" } ], [], [] ] } Infiles json block is an array of 3. The first value is always relative to the current product. The second and third values can be used to reference other products, such as the previous in the timeserie or the first of the timeserie (e.g: to have a unique reference for all the images in the timeserie). For instance, it is useful in order to process coherence cf. Example with S1","title":"Example"},{"location":"user-guide/graph/#output-files","text":"OutFile defines a file (name ( Layer ) and extension) generated by the processing steps. Output files can be stored in the storage, indexed in the geocube (for processor only), or flagged as to be deleted. If an OutFile is to be indexed in the Geocube, the following information must be provided (for other action, they will be ignored): - DType - NoData - Min , Max - ExtMin , ExtMax - Exponent - Nbands See geocube indexation documentation for further information.","title":"Output Files"},{"location":"user-guide/graph/#structure_2","text":"// OutFile describes an output file of the processing type OutFile struct { File dformatOut Arg // argFixed or argConfig DType DType `json:\"datatype\"` NoData float64 `json:\"nodata\"` Min float64 `json:\"min_value\"` Max float64 `json:\"max_value\"` ExtMin float64 `json:\"ext_min_value\"` ExtMax float64 `json:\"ext_max_value\"` Exponent float64 `json:\"exponent\"` // JSON default: 1 Nbands int `json:\"nbands\"` // JSON default: 1 Action OutFileAction `json:\"action\"` Condition TileCondition `json:\"condition\"` // JSON default: pass }","title":"Structure"},{"location":"user-guide/graph/#available-action","text":"// OutFileAction const ( ToIgnore OutFileAction = iota `json:\"to_ignore\"` ToCreate `json:\"to_create\"` ToIndex `json:\"to_index\"` ToDelete `json:\"to_delete\"` )","title":"Available action"},{"location":"user-guide/graph/#example_1","text":"{ \"out_files\":[ [ { \"layer\":\"img\", \"extension\":\"tif\", \"action\":\"to_index\", \"dformat_out\":{ \"type\":\"fixed\", \"value\":\"uint16,0,0,30000\" }, \"ext_min_value\":0, \"ext_max_value\":3, \"nbands\":4 }, { \"layer\":\"clcsh\", \"extension\":\"tif\", \"action\":\"to_create\", }, { \"layer\":\"__product__\", \"extension\":\"SAFE\", \"action\":\"to_delete\", \"error_condition\": \"on_fatal_failure\" } ], [], [] ] }","title":"Example"},{"location":"user-guide/graph/#steps","text":"List of processing steps that will be executed sequentially.","title":"Steps"},{"location":"user-guide/graph/#engine","text":"Define the kind of engine that will be used to execute the step. Available engines: Snap: for Sentinel constellation (if installed) Python: to execute python script Docker: to run a docker Cmd: for command available in the docker","title":"Engine"},{"location":"user-guide/graph/#command","text":"Define the command to execute with the given engine. MyBinary Example: { \"processing_steps\": [ { \"engine\": \"cmd\", \"command\": \"./cmd/MyBinary\" } ] } Docker Example: { \"engine\": \"docker\", \"command\": \"containerRegistry/myImage:myTag\" }","title":"Command"},{"location":"user-guide/graph/#args","text":"List of arguments that will be passed to the command, with the following synthax : --argument-name argument-value . They can be of five types: ArgIn : The path of the file corresponding to a layer in InFiles . ArgOut : The path of the file corresponding to a layer in OutFiles . ArgFixed : A fixed value ArgConfig : A value retrieved from the Config using a key. The key workdir is defined at runtime to point to the working directory. ArgTile : A value retrieved from the tile. Supported value: constellation scene name date number : tile number (for Sentinel-1: =burst number) swath (Sentinel-1) cohdate : (Sentinel-1) Date of the reference burst if different from previous date or date of the burst","title":"Args"},{"location":"user-guide/graph/#structure_3","text":"type ArgIn struct { Input int `json:\"tile_index\"` // Index of input [0, 1, 2] Layer service.Layer `json:\"layer\"` Extension service.Extension `json:\"extension\"` } type ArgOut struct { service.Layer `json:\"layer\"` Extension service.Extension `json:\"extension\"` } type ArgFixed string // fixed arg type ArgConfig string // arg from config type ArgTile string // arg from tile info","title":"Structure:"},{"location":"user-guide/graph/#example_2","text":"{ \"engine\":\"docker\", \"command\":\"containerRegistry/myImage:myTag\", \"args\":{ \"workdir\":{ \"type\":\"config\", \"value\":\"workdir\" }, \"image-in\":{ \"type\":\"in\", \"layer\":\"__product__\", \"extension\":\"SAFE\" }, \"parameter-from-config\":{ \"type\":\"config\", \"value\":\"key-in-config\" }, \"constellation\":{ \"type\":\"fixed\", \"value\":\"sentinel2\" }, \"out-file\":{ \"type\":\"out\", \"layer\":\"img\", \"extension\":\"tif\" }, \"out-pattern\":{ \"type\":\"out\", \"layer\":\"*\", \"extension\":\"tif\" } } } This step will run something similar to: docker run -v <volume>:<workdir> containerRegistry/myImage:myTag --workdir <config['workdir']> --image-in <path to in_file[0]/product.SAFE> --parameter-from-config <config['key-in-config']> --constellation sentinel2 --out-file <path to out_file[0]/img.tif> --out-pattern <path to out_file[0]>/*.tif","title":"Example"},{"location":"user-guide/graph/#condition","text":"Condition to optionally execute the step.","title":"Condition"},{"location":"user-guide/graph/#structure_4","text":"type TileCondition struct { Name string Pass func([]common.Tile) bool }","title":"Structure"},{"location":"user-guide/graph/#available-tileconditions","text":"// condPass is a tileCondition always true var pass = TileCondition{\"pass\", func(tiles []common.Tile) bool { return true }} // condDiffTile returns true if tile1 != tile2 var condDiffT0T1 = TileCondition{\"different_T0_T1\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID != tiles[1].Scene.SourceID }} var condDiffT0T2 = TileCondition{\"different_T0_T2\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID != tiles[2].Scene.SourceID }} var condDiffT1T2 = TileCondition{\"different_T1_T2\", func(tiles []common.Tile) bool { return tiles[1].Scene.SourceID != tiles[2].Scene.SourceID }} // condEqualTile returns true if tile1 == tile2 var condEqualT0T1 = TileCondition{\"equal_T0_T1\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID == tiles[1].Scene.SourceID }} var condEqualT0T2 = TileCondition{\"equal_T0_T2\", func(tiles []common.Tile) bool { return tiles[0].Scene.SourceID == tiles[2].Scene.SourceID }} var condEqualT1T2 = TileCondition{\"equal_T1_T2\", func(tiles []common.Tile) bool { return tiles[1].Scene.SourceID == tiles[2].Scene.SourceID }}","title":"Available tileConditions"},{"location":"user-guide/graph/#example_3","text":"{ \"condition\": \"pass\" }","title":"Example"},{"location":"user-guide/graph/#example-processing-sentinel1","text":"{ \"config\": { \"bs_erode_iterations\": \"10\", \"coh_erode_iterations\": \"10\", \"coherence_azimuth\": \"4\", \"coherence_range\": \"16\", \"dem_egm_correction\": \"True\", \"dem_file\": \"\", \"dem_name\": \"SRTM 3Sec\", \"dem_nodata\": \"0\", \"dem_resampling\": \"BILINEAR_INTERPOLATION\", \"dformat_out\": \"float32,0,0,1\", \"img_resampling\": \"BICUBIC_INTERPOLATION\", \"bkg_resampling\": \"BISINC_21_POINT_INTERPOLATION\", \"projection\": \"EPSG:4326\", \"resolution\": \"20\", \"snap_cpu_parallelism\": \"1\", \"terrain_correction_azimuth\": \"1\", \"terrain_correction_range\": \"4\" }, \"processing_steps\": [ { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_BkG.xml\", \"args\": { \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"resampling\": { \"type\": \"config\", \"value\": \"bkg_resampling\" }, \"master\": { \"type\": \"in\", \"tile_index\": 2, \"layer\": \"preprocessed\", \"extension\": \"dim\" }, \"output\": { \"type\": \"out\", \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"slave\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"preprocessed\", \"extension\": \"dim\" } }, \"condition\": \"pass\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_SlvExtract.xml\", \"args\": { \"input\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"output\": { \"type\": \"out\", \"layer\": \"coregextract\", \"extension\": \"dim\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_Deb_BetaSigma_ML_TC_RNKELL.xml\", \"args\": { \"azimuth_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_azimuth\" }, \"band\": { \"type\": \"fixed\", \"value\": \"Sigma0\" }, \"dem_egm\": { \"type\": \"config\", \"value\": \"dem_egm_correction\" }, \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"grid_align\": { \"type\": \"fixed\", \"value\": \"true\" }, \"img_resampling\": { \"type\": \"config\", \"value\": \"img_resampling\" }, \"img_suffix\": { \"type\": \"tile\", \"value\": \"date\" }, \"input\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"outputVH\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"outputVV\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"projection\": { \"type\": \"config\", \"value\": \"projection\" }, \"range_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_range\" }, \"resolution\": { \"type\": \"config\", \"value\": \"resolution\" }, \"swath\": { \"type\": \"tile\", \"value\": \"swath\" }, \"trig\": { \"type\": \"fixed\", \"value\": \"sin\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"bs_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VV\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"bs_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"pass\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"sigma0_VH\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"pass\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_BkG.xml\", \"args\": { \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"resampling\": { \"type\": \"config\", \"value\": \"bkg_resampling\" }, \"master\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregextract\", \"extension\": \"dim\" }, \"output\": { \"type\": \"out\", \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"slave\": { \"type\": \"in\", \"tile_index\": 1, \"layer\": \"coregextract\", \"extension\": \"dim\" } }, \"condition\": \"different_T1_T2\" }, { \"engine\": \"snap\", \"command\": \"snap/S1_SLC_Coh_BSel_Deb_ML_TC.xml\", \"args\": { \"azimuth_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_azimuth\" }, \"coherence_azimuth\": { \"type\": \"config\", \"value\": \"coherence_azimuth\" }, \"coherence_range\": { \"type\": \"config\", \"value\": \"coherence_range\" }, \"dem_egm\": { \"type\": \"config\", \"value\": \"dem_egm_correction\" }, \"dem_file\": { \"type\": \"config\", \"value\": \"dem_file\" }, \"dem_name\": { \"type\": \"config\", \"value\": \"dem_name\" }, \"dem_nodata\": { \"type\": \"config\", \"value\": \"dem_nodata\" }, \"dem_resampling\": { \"type\": \"config\", \"value\": \"dem_resampling\" }, \"grid_align\": { \"type\": \"fixed\", \"value\": \"true\" }, \"img_resampling\": { \"type\": \"config\", \"value\": \"img_resampling\" }, \"input\": { \"type\": \"in\", \"tile_index\": 0, \"layer\": \"coregistred\", \"extension\": \"dim\" }, \"outputVH\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"outputVV\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"projection\": { \"type\": \"config\", \"value\": \"projection\" }, \"range_multilook\": { \"type\": \"config\", \"value\": \"terrain_correction_range\" }, \"resolution\": { \"type\": \"config\", \"value\": \"resolution\" }, \"sel_date\": { \"type\": \"tile\", \"value\": \"cohdate\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"coh_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VV\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/erodeMask.py\", \"args\": { \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"iterations\": { \"type\": \"config\", \"value\": \"coh_erode_iterations\" }, \"no-data\": { \"type\": \"fixed\", \"value\": \"0\" } }, \"condition\": \"different_T0_T1\" }, { \"engine\": \"python\", \"command\": \"python/convert.py\", \"args\": { \"dformat-out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"file-in\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"file-out\": { \"type\": \"out\", \"layer\": \"coh_VH\", \"extension\": \"tif\" }, \"range-in\": { \"type\": \"fixed\", \"value\": \"0,1\" } }, \"condition\": \"different_T0_T1\" } ], \"in_files\": [ [ { \"layer\": \"preprocessed\", \"extension\": \"dim\" } ], [ { \"layer\": \"coregextract\", \"extension\": \"dim\", \"condition\": \"different_T1_T2\" } ], [ { \"layer\": \"preprocessed\", \"extension\": \"dim\", \"condition\": \"different_T0_T2\" } ] ], \"out_files\": [ [ { \"layer\": \"sigma0_VV\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\" }, { \"layer\": \"sigma0_VH\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\" }, { \"layer\": \"coh_VV\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\", \"condition\": \"different_T0_T1\" }, { \"layer\": \"coh_VH\", \"extension\": \"tif\", \"dformat_out\": { \"type\": \"config\", \"value\": \"dformat_out\" }, \"ext_min_value\": 0, \"ext_max_value\": 1, \"action\": \"to_index\", \"condition\": \"different_T0_T1\" } ], [ ], [] ] }","title":"Example Processing Sentinel1"},{"location":"user-guide/graph/#example-processing-sentinel2","text":"{ \"config\": { \"srtm-uri\": \"...\" }, \"processing_steps\": [ { \"engine\": \"cmd\", \"command\": \"./cmd/radiometric_processor\", \"args\": { \"workdir\": { \"type\": \"config\", \"value\": \"workdir\" }, \"image\": { \"type\": \"in\", \"layer\": \"__product__\", \"extension\": \"SAFE\" }, \"srtm-uri\": { \"type\": \"config\", \"value\": \"srtm-uri\" }, \"constellation\": { \"type\": \"fixed\", \"value\": \"sentinel2\" }, \"out-pattern\": { \"type\": \"out\", \"layer\": \"*\", \"extension\": \"tif\" } }, \"condition\": \"pass\" }, { \"engine\": \"cmd\", \"command\": \"./cmd/registation\", \"args\": { \"workdir\": { \"type\": \"config\", \"value\": \"workdir\" }, \"satellite-image\": { \"type\": \"in\", \"layer\": \"__product__\", \"extension\": \"SAFE\" }, \"input\": { \"type\": \"fixed\", \"value\": \"*.tif\"}, \"output\": { \"type\": \"fixed\", \"value\": \".\" }, \"constellation\": { \"type\": \"fixed\", \"value\": \"Sentinel2\" } }, \"condition\": \"pass\" } ], \"in_files\": [ [ { \"layer\": \"__product__\", \"extension\": \"SAFE\" } ], [], [] ], \"out_files\": [ [ { \"layer\": \"img\", \"extension\": \"tif\", \"action\": \"to_index\", \"dformat_out\": { \"type\": \"fixed\", \"value\": \"uint16,0,0,30000\" }, \"ext_min_value\": 0, \"ext_max_value\": 3, \"nbands\": 4 }, { \"layer\": \"quality_mask\", \"extension\": \"tif\", \"action\": \"to_index\", \"dformat_out\": { \"type\": \"fixed\", \"value\": \"int16,-1,0,10000\" }, \"ext_min_value\": 0, \"ext_max_value\": 1 } ], [], [] ] }","title":"Example Processing Sentinel2"},{"location":"user-guide/graph/#docker-engine","text":"","title":"Docker Engine"},{"location":"user-guide/graph/#principle-concept","text":"Instead of using and runing one large image docker which contains every dependencies (Snap, Python, other binaries), the ingester processor can be a docker container orchestrator. Processor Replication controller can be configured to run two containers: Generic container with processor Go binary and docker binary (Client) Dind (docker in docker) container which contains docker daemon (Server)","title":"Principle &amp; Concept"},{"location":"user-guide/graph/#deployment","text":"In order to deploy this kind of architecture, some configuration must be added in your k8s config file: In client container (environment var): - name: DOCKER_HOST value: tcp://localhost:2375 A certificate can be defined in order to secure your server or enable insecure trafic. In Server container (environment var): - name: DOCKER_TLS_CERTDIR value: \"\"","title":"Deployment"},{"location":"user-guide/graph/#private-registry","text":"In order to use private registry, credentials must be configured in Client side container. user (for gcp registry: _json_key ) password (for gcp registry: service account content) server (for gcp registry e.g: https://europe-west1-docker.pkg.dev ) For images stored in public registry (ex. docker.hub), there is no need for credentials.","title":"Private Registry"},{"location":"user-guide/graph/#credentials","text":"Some containers need file credentials (to request storage for example). docker-mount-volumes can be defined in client executable arguments to mount a volume containing credential file. Variable environment must be also defined in graph file: { \"engine\": \"docker\", \"command\": \"myImage\", \"args\": { \"workdir\": { \"type\": \"config\", \"value\": \"workdir\" }, \"image\": { \"type\": \"in\", \"layer\": \"__product__\", \"extension\": \"SAFE\" }, \"srtm-uri\": { \"type\": \"config\", \"value\": \"srtm-uri\" }, \"roadmap\": { \"type\": \"config\", \"value\": \"roadmap-robot\" }, \"options\": { \"type\": \"config\", \"value\": \"roadmap-options\" }, \"roadmap-params\": { \"type\": \"config\", \"value\": \"roadmap-params\" }, \"libs\": { \"type\": \"config\", \"value\": \"libs\" }, \"constellation\": { \"type\": \"fixed\", \"value\": \"sentinel2\" }, \"out-pattern\": { \"type\": \"out\", \"layer\": \"*\", \"extension\": \"tif\" } }, \"envs\": [\"GOOGLE_APPLICATION_CREDENTIALS=/myPath/myFile\"], \"condition\": \"pass\" } Important: Path defined in args and path in graphFile should be equal.","title":"Credentials"},{"location":"user-guide/graph/#volume","text":"In order to mount volume correctly, it is important to enable access to all containers (processor & docker-daemon) Example: If you want to use a workdir which is mount on /local-sdd in your processor kubernetes deployment. If you want to mount also this workdir on your docker processing, you need to mount this volume on processor container and docker-dind container to enable docker-daemon access.","title":"Volume"},{"location":"user-guide/monitoring/","text":"Monitoring To monitor an ingestion, the workflow server provides several endpoints. Endpoints AOI GET /aoi/ : List all the AOIS POST /aoi/{aoi} : create a new AOI POST /aoi/{aoi}/scene : add a new scene and its tiles to the graph of dependencies PUT /aoi/{aoi}/retry : retry all the scenes and tiles of the AOI (iif Status=RETRY) GET /aoi/{aoi} : overview of the workload for an AOI Scenes: new: 0 pending: 10 done: 36 retry: 0 failed: 0 Total: 46 Tiles: new: 10 pending: 21 done: 15 retry: 0 failed: 0 Total: 46 Root tiles : 46 From: 2022-01-04 To: 2022-10-12 GET /aoi/{aoi}/dot : Pretty display of the workflow Ex: Scenes Monitoring endpoints concerning Scenes are returned in GeoJSON Format. We can found a lot of information such as geometry, product tags and payload information. GET /aoi/{aoi}/scenes : list Scenes of an AOI Ex: [ { \"id\": 12, \"source_id\": \"S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507\", \"aoi\": \"DenmarkDemoS2\", \"data\": { \"uuid\": \"4c0e52b5-c96d-5bfc-b0f0-c78ed9c50d48\", \"date\": \"2022-01-04T10:34:31.024Z\", \"tiles\": { \"S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507\": { \"swath_id\": \"\", \"tile_nr\": 0 } }, \"graph_name\": \"CopyProductToStorage\", \"graph_config\": {}, \"record_id\": \"ea343a2a-4189-466c-9519-eb38b908ab13\", \"instances_id\": { \"B01\": \"7b2cb808-a021-4de1-8703-716824fc23d8\", \"B02\": \"7b750eec-692a-45d0-ada9-92beb26e9f60\", \"B03\": \"7044c9d4-547e-4f22-8eeb-d6fbecc8b764\", \"B04\": \"cdce88a6-32b4-4b1e-af5e-c1e2f274cf2e\", \"B05\": \"b902d419-56d2-461e-b436-a8248cd10ff1\", \"B06\": \"82187d20-b58c-4324-a113-7b8ebd1b8cf7\", \"B07\": \"10c6f347-ed88-45ab-8c05-e434167d31f2\", \"B08\": \"23e10257-4d57-4094-b96e-ecc484671f68\", \"B09\": \"4274efb4-f2c4-4132-9905-a8f9d6accad1\", \"B10\": \"5eb73278-ce79-4374-a5c7-77c269f8e6e1\", \"B11\": \"f910e281-f870-487e-9b58-a8768ef1ba07\", \"B12\": \"613094ff-a87e-4107-bb29-a84f05ccea88\" }, \"metadata\": null, \"is_retriable\": true, \"storage_uri\": \"\" }, \"status\": \"DONE\", \"message\": \"\", \"RetryCountDown\": -1 }, { \"id\": 14, \"source_id\": \"S2B_MSIL1C_20220112T104319_N0510_R008_T32UNG_20240428T024837\", \"aoi\": \"DenmarkDemoS2\", \"data\": { \"uuid\": \"d16f8a29-cabc-48c2-a569-3a3d633dc2c4\", \"date\": \"2022-01-12T10:43:19Z\", \"tiles\": { \"S2B_MSIL1C_20220112T104319_N0510_R008_T32UNG_20240428T024837\": { \"swath_id\": \"\", \"tile_nr\": 0 } }, \"graph_name\": \"CopyProductToStorage\", \"graph_config\": {}, \"record_id\": \"70439ab1-b2c8-433a-a3ff-f5dfa0249c11\", \"instances_id\": { \"B01\": \"7b2cb808-a021-4de1-8703-716824fc23d8\", \"B02\": \"7b750eec-692a-45d0-ada9-92beb26e9f60\", \"B03\": \"7044c9d4-547e-4f22-8eeb-d6fbecc8b764\", \"B04\": \"cdce88a6-32b4-4b1e-af5e-c1e2f274cf2e\", \"B05\": \"b902d419-56d2-461e-b436-a8248cd10ff1\", \"B06\": \"82187d20-b58c-4324-a113-7b8ebd1b8cf7\", \"B07\": \"10c6f347-ed88-45ab-8c05-e434167d31f2\", \"B08\": \"23e10257-4d57-4094-b96e-ecc484671f68\", \"B09\": \"4274efb4-f2c4-4132-9905-a8f9d6accad1\", \"B10\": \"5eb73278-ce79-4374-a5c7-77c269f8e6e1\", \"B11\": \"f910e281-f870-487e-9b58-a8768ef1ba07\", \"B12\": \"613094ff-a87e-4107-bb29-a84f05ccea88\" }, \"metadata\": null, \"is_retriable\": true, \"storage_uri\": \"\" }, \"status\": \"FAILED\", \"message\": \"ProcessScene.ImageProviders.URLImageProvider: unable to retrieve download Link: scene metadata is empty\\n Product not found or unavailable: ...\", \"RetryCountDown\": -1 } ] GET /aoi/{aoi}/scenes/{status} : get Scenes of an AOI filtered by Status GET /scene/{scene} : get Scene using its id PUT /scene/{scene}/retry : retry the scene (if scene.Status=RETRY) PUT /scene/{scene}/fail : tag the scene and all its tiles as failed and update the graph of dependencies (if scene.Status=RETRY if /force is not stated) Tiles Monitoring endpoints concerning Tiles are returned in GeoJSON Format. We can found a lot of information such as geometry, product tags and payload information. GET /scene/{scene}/tiles : get tiles of Scene (GeoJSON Format) Ex: [ { \"id\": 12, \"source_id\": \"S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507\", \"scene\": { \"id\": 12, \"source_id\": \"\", \"aoi\": \"\", \"data\": { \"uuid\": \"\", \"date\": \"0001-01-01T00:00:00Z\", \"graph_name\": \"\", \"graph_config\": null, \"record_id\": \"\", \"metadata\": null, \"is_retriable\": false, \"storage_uri\": \"\" } }, \"data\": { \"swath_id\": \"\", \"tile_nr\": 0, \"graph_name\": \"library/graph/ExtractS2Bands.json\", \"is_retriable\": true }, \"status\": \"RETRY\", \"message\": \"ProcessTile[20220104_S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507].LoadGraphFromFile.stat library/graph/ExtractS2Bands.json: no such file or directory\\n badly formatted storage uri\", \"PreviousID\": null, \"ReferenceID\": null, \"RetryCountDown\": -1 } ] GET /tile/{tile} : get Tile using id GET /aoi/{aoi}/tiles/{status} : get Tiles of an AOI filtered by Status PUT /tile/{tile}/retry : retry the tile (iif tile.Status=RETRY) PUT /tile/{tile}/fail : tag the tile as failed and update the graph of dependencies (iif tile.Status=RETRY if /force is not stated) User-interface A very ugly, but useful HTML interface can be found here tools/workflow/main.html .","title":"Monitoring"},{"location":"user-guide/monitoring/#monitoring","text":"To monitor an ingestion, the workflow server provides several endpoints.","title":"Monitoring"},{"location":"user-guide/monitoring/#endpoints","text":"","title":"Endpoints"},{"location":"user-guide/monitoring/#aoi","text":"GET /aoi/ : List all the AOIS POST /aoi/{aoi} : create a new AOI POST /aoi/{aoi}/scene : add a new scene and its tiles to the graph of dependencies PUT /aoi/{aoi}/retry : retry all the scenes and tiles of the AOI (iif Status=RETRY) GET /aoi/{aoi} : overview of the workload for an AOI Scenes: new: 0 pending: 10 done: 36 retry: 0 failed: 0 Total: 46 Tiles: new: 10 pending: 21 done: 15 retry: 0 failed: 0 Total: 46 Root tiles : 46 From: 2022-01-04 To: 2022-10-12 GET /aoi/{aoi}/dot : Pretty display of the workflow Ex:","title":"AOI"},{"location":"user-guide/monitoring/#scenes","text":"Monitoring endpoints concerning Scenes are returned in GeoJSON Format. We can found a lot of information such as geometry, product tags and payload information. GET /aoi/{aoi}/scenes : list Scenes of an AOI Ex: [ { \"id\": 12, \"source_id\": \"S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507\", \"aoi\": \"DenmarkDemoS2\", \"data\": { \"uuid\": \"4c0e52b5-c96d-5bfc-b0f0-c78ed9c50d48\", \"date\": \"2022-01-04T10:34:31.024Z\", \"tiles\": { \"S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507\": { \"swath_id\": \"\", \"tile_nr\": 0 } }, \"graph_name\": \"CopyProductToStorage\", \"graph_config\": {}, \"record_id\": \"ea343a2a-4189-466c-9519-eb38b908ab13\", \"instances_id\": { \"B01\": \"7b2cb808-a021-4de1-8703-716824fc23d8\", \"B02\": \"7b750eec-692a-45d0-ada9-92beb26e9f60\", \"B03\": \"7044c9d4-547e-4f22-8eeb-d6fbecc8b764\", \"B04\": \"cdce88a6-32b4-4b1e-af5e-c1e2f274cf2e\", \"B05\": \"b902d419-56d2-461e-b436-a8248cd10ff1\", \"B06\": \"82187d20-b58c-4324-a113-7b8ebd1b8cf7\", \"B07\": \"10c6f347-ed88-45ab-8c05-e434167d31f2\", \"B08\": \"23e10257-4d57-4094-b96e-ecc484671f68\", \"B09\": \"4274efb4-f2c4-4132-9905-a8f9d6accad1\", \"B10\": \"5eb73278-ce79-4374-a5c7-77c269f8e6e1\", \"B11\": \"f910e281-f870-487e-9b58-a8768ef1ba07\", \"B12\": \"613094ff-a87e-4107-bb29-a84f05ccea88\" }, \"metadata\": null, \"is_retriable\": true, \"storage_uri\": \"\" }, \"status\": \"DONE\", \"message\": \"\", \"RetryCountDown\": -1 }, { \"id\": 14, \"source_id\": \"S2B_MSIL1C_20220112T104319_N0510_R008_T32UNG_20240428T024837\", \"aoi\": \"DenmarkDemoS2\", \"data\": { \"uuid\": \"d16f8a29-cabc-48c2-a569-3a3d633dc2c4\", \"date\": \"2022-01-12T10:43:19Z\", \"tiles\": { \"S2B_MSIL1C_20220112T104319_N0510_R008_T32UNG_20240428T024837\": { \"swath_id\": \"\", \"tile_nr\": 0 } }, \"graph_name\": \"CopyProductToStorage\", \"graph_config\": {}, \"record_id\": \"70439ab1-b2c8-433a-a3ff-f5dfa0249c11\", \"instances_id\": { \"B01\": \"7b2cb808-a021-4de1-8703-716824fc23d8\", \"B02\": \"7b750eec-692a-45d0-ada9-92beb26e9f60\", \"B03\": \"7044c9d4-547e-4f22-8eeb-d6fbecc8b764\", \"B04\": \"cdce88a6-32b4-4b1e-af5e-c1e2f274cf2e\", \"B05\": \"b902d419-56d2-461e-b436-a8248cd10ff1\", \"B06\": \"82187d20-b58c-4324-a113-7b8ebd1b8cf7\", \"B07\": \"10c6f347-ed88-45ab-8c05-e434167d31f2\", \"B08\": \"23e10257-4d57-4094-b96e-ecc484671f68\", \"B09\": \"4274efb4-f2c4-4132-9905-a8f9d6accad1\", \"B10\": \"5eb73278-ce79-4374-a5c7-77c269f8e6e1\", \"B11\": \"f910e281-f870-487e-9b58-a8768ef1ba07\", \"B12\": \"613094ff-a87e-4107-bb29-a84f05ccea88\" }, \"metadata\": null, \"is_retriable\": true, \"storage_uri\": \"\" }, \"status\": \"FAILED\", \"message\": \"ProcessScene.ImageProviders.URLImageProvider: unable to retrieve download Link: scene metadata is empty\\n Product not found or unavailable: ...\", \"RetryCountDown\": -1 } ] GET /aoi/{aoi}/scenes/{status} : get Scenes of an AOI filtered by Status GET /scene/{scene} : get Scene using its id PUT /scene/{scene}/retry : retry the scene (if scene.Status=RETRY) PUT /scene/{scene}/fail : tag the scene and all its tiles as failed and update the graph of dependencies (if scene.Status=RETRY if /force is not stated)","title":"Scenes"},{"location":"user-guide/monitoring/#tiles","text":"Monitoring endpoints concerning Tiles are returned in GeoJSON Format. We can found a lot of information such as geometry, product tags and payload information. GET /scene/{scene}/tiles : get tiles of Scene (GeoJSON Format) Ex: [ { \"id\": 12, \"source_id\": \"S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507\", \"scene\": { \"id\": 12, \"source_id\": \"\", \"aoi\": \"\", \"data\": { \"uuid\": \"\", \"date\": \"0001-01-01T00:00:00Z\", \"graph_name\": \"\", \"graph_config\": null, \"record_id\": \"\", \"metadata\": null, \"is_retriable\": false, \"storage_uri\": \"\" } }, \"data\": { \"swath_id\": \"\", \"tile_nr\": 0, \"graph_name\": \"library/graph/ExtractS2Bands.json\", \"is_retriable\": true }, \"status\": \"RETRY\", \"message\": \"ProcessTile[20220104_S2A_MSIL1C_20220104T103431_N0301_R108_T32UNG_20220104T123507].LoadGraphFromFile.stat library/graph/ExtractS2Bands.json: no such file or directory\\n badly formatted storage uri\", \"PreviousID\": null, \"ReferenceID\": null, \"RetryCountDown\": -1 } ] GET /tile/{tile} : get Tile using id GET /aoi/{aoi}/tiles/{status} : get Tiles of an AOI filtered by Status PUT /tile/{tile}/retry : retry the tile (iif tile.Status=RETRY) PUT /tile/{tile}/fail : tag the tile as failed and update the graph of dependencies (iif tile.Status=RETRY if /force is not stated)","title":"Tiles"},{"location":"user-guide/monitoring/#user-interface","text":"A very ugly, but useful HTML interface can be found here tools/workflow/main.html .","title":"User-interface"},{"location":"user-guide/payload/","text":"Payload The input of the ingester is a payload. It contains an AOI, a date interval, parameters defining the raw products, parameters defining the processing and parameters defining the products to be ingested in the Geocube. The payload will be used to: 1. List available scenes corresponding to the criterias ( Catalogue ) 2. Configure the downloading and pre-processing of the scenes (Downloader) 3. Configure the processing of the scenes (Processor) 4. Index the output products (Processor) The payload is a GeoJSON (all fields are mandatory unless otherwise stated): AOI according to GeoJSON standards ( type , features , geometry , coordinates ...) name : Unique name used to identify the Area in the workflow. After a first ingestion, new scenes can be added to the same area, benefiting from automatic scenes reference picking (useful for S1-bursts). start_time , end_time : date interval scene_type : describing the type of the products to be downloaded constellation : Name of the Satellite Constellation (see currently supported ) parameters : (optional) specific parameters to filter the results (see Catalogue API guide ) scene_graph_name : name of the graph (or \"CopyProductToStorage\") that will be used just after downloading the scene (see Processing Graphs ) tile_graph_name : name of the graph (or \"Pass\") that will be used to process each tile (output of the downloader) (see Processing Graphs ) graph_config : (optional): specific configuration of the graphs layers : mapping between layers to be indexed in the Geocube and the corresponding variable.instance from the Geocube (see Geocube Documentation). layername : {\"variable\":\"variable_name\", \"instance\":\"instance_name\"} record_tags (optional): user-defined tags for identifying/creating the record in the Geocube. annotations_urls (optional): list of urls to retrieve Sentinel-1 annotations is_retriable (optional): define if the processing or download is retriable if a fatal error occurs (or if retry_count is over) retry_count (optional): define the number of time a processing or download is retried if a transient error occurs storage_uri (optional): define a custom storage page , limit (optional): query the n-th page (0-based) of the catalog and return limit scenes at most. Using page , limit , the number of scenes returned might be less than limit even if it's not the last page. Check properties[\"next\"] for another page (value true / false ) Parameters to request the catalogue to find the products To list available scenes covering an AOI between a range of time, the Catalogue ) will use the fields geometry , start_time , end_time and scene_type . The scene_type defines the consellation and the parameters depends on the catalogue: Copernicus Available scene_type/parameters : platformname : e.g. SENTINEL-1 , SENTINEL-2 productType : e.g. SLC , S2MSI1C filename For Sentinel-1 only: polarisationmode : default VV VH sensoroperationalmode : default IW relativeorbitnumber For optical only: cloudcoverpercentage : format [min TO max] Landsat Available scene_type/parameters : cloudcoverpercentage : format [min TO max] OneAtlas OneAtlas available parameters: platform : e.g. PHR1B acquisitionIdentifier : e.g. ACQ_PNEO4_02426805581362 productType : e.g. bundle processingLevel cloudCover : e.g. [0,10] incidenceAngle workspace relation NB: To use other parameters or mix different kind of imagery. You need to group all your images in the same workspace (via OneAtlas) and reference only workspace in payload ingestion. For more information see: OneAtlas API guide Parameters to download and process the products scene_graph_name defines a path to a processing graph that will be used by the downloader to pre-process the products (example: extract every burst from scenes). tile_graph_name defines a path to a processing graph that will be used by the processor to process the tiles. graph_config defines pairs of key-values that can be used by the preceding graphs (see Graph:Args ) Sentinel1 scene_graph_name is usually used to extract the bursts from the Scenes. This step is done by the downloader as a pre-processing (1 job per Scene). tile_graph_name defines the processing to do for every Tiles. This step is done by the processor (1 job per Tile). Sentinel2, SPOT, PHR scene_graph_name can be used to pre-process data (example: extract Panchromatic & MultiSpectral Image from DIMAP product) but it usually used to copy the data to the ingester storage ( =CopyToStorage ). In this case a Tile is the whole Scene. tile_graph_name defines the processing to do for each Tile. This step is done by the processor. cf. Payload ingestion Parameters to index the output products record_tags will be used by the catalogue, just before the ingestion starts, to create the records in the Geocube to index the output products layers will define the match between the output layers of the graph and the variables (and instances) of the Geocube (to index the data). Example of an AOI Payload { \"name\":\"DenmarkDemo\", \"type\":\"Feature\", \"geometry\":{ \"type\": \"MultiPolygon\", \"coordinates\": [ [ [ [10.061230468750068, 54.88637695312502], [9.957128906249977, 54.87246093750002], [9.903906250000063, 54.896630859374994], [9.80625, 54.90600585937503], [9.77119140625004, 55.059912109375034], [9.78125, 55.06904296875001], [9.830371093750015, 55.05825195312505], [9.998828125000045, 54.986474609374994], [10.05771484375006, 54.90791015624998], [10.061230468750068, 54.88637695312502] ] ] ] }, \"start_time\":\"2022-01-01T00:00:00.000Z\", \"end_time\":\"2022-02-10T00:00:00.000Z\", \"scene_type\":{ \"constellation\":\"sentinel1\", \"parameters\": { \"producttype\": \"SLC\", \"polarisationmode\": \"VV VH\", \"sensoroperationalmode\": \"IW\", \"relativeorbitnumber\": \"44\" } }, \"scene_graph_name\":\"S1Preprocessing\", \"tile_graph_name\":\"S1BackscatterCoherence\", \"graph_config\":{ \"projection\":\"EPSG:32632\", \"snap_cpu_parallelism\":\"8\", \"bkg_resampling\": \"BISINC_5_POINT_INTERPOLATION\" }, \"layers\":{ \"sigma0_VV\": {\"variable\":\"BackscatterSigma0VV\", \"instance\":\"RNKell\"}, \"sigma0_VH\": {\"variable\":\"BackscatterSigma0VH\", \"instance\":\"RNKell\"}, \"coh_VV\": {\"variable\":\"CoherenceVV\", \"instance\":\"master\"}, \"coh_VH\": {\"variable\":\"CoherenceVH\", \"instance\":\"master\"} }, \"record_tags\":{ \"source\": \"tutorial\", \"provider\": \"geocube-ingester\", \"area\":\"Denmark\" } }","title":"Payload"},{"location":"user-guide/payload/#payload","text":"The input of the ingester is a payload. It contains an AOI, a date interval, parameters defining the raw products, parameters defining the processing and parameters defining the products to be ingested in the Geocube. The payload will be used to: 1. List available scenes corresponding to the criterias ( Catalogue ) 2. Configure the downloading and pre-processing of the scenes (Downloader) 3. Configure the processing of the scenes (Processor) 4. Index the output products (Processor) The payload is a GeoJSON (all fields are mandatory unless otherwise stated): AOI according to GeoJSON standards ( type , features , geometry , coordinates ...) name : Unique name used to identify the Area in the workflow. After a first ingestion, new scenes can be added to the same area, benefiting from automatic scenes reference picking (useful for S1-bursts). start_time , end_time : date interval scene_type : describing the type of the products to be downloaded constellation : Name of the Satellite Constellation (see currently supported ) parameters : (optional) specific parameters to filter the results (see Catalogue API guide ) scene_graph_name : name of the graph (or \"CopyProductToStorage\") that will be used just after downloading the scene (see Processing Graphs ) tile_graph_name : name of the graph (or \"Pass\") that will be used to process each tile (output of the downloader) (see Processing Graphs ) graph_config : (optional): specific configuration of the graphs layers : mapping between layers to be indexed in the Geocube and the corresponding variable.instance from the Geocube (see Geocube Documentation). layername : {\"variable\":\"variable_name\", \"instance\":\"instance_name\"} record_tags (optional): user-defined tags for identifying/creating the record in the Geocube. annotations_urls (optional): list of urls to retrieve Sentinel-1 annotations is_retriable (optional): define if the processing or download is retriable if a fatal error occurs (or if retry_count is over) retry_count (optional): define the number of time a processing or download is retried if a transient error occurs storage_uri (optional): define a custom storage page , limit (optional): query the n-th page (0-based) of the catalog and return limit scenes at most. Using page , limit , the number of scenes returned might be less than limit even if it's not the last page. Check properties[\"next\"] for another page (value true / false )","title":"Payload"},{"location":"user-guide/payload/#parameters-to-request-the-catalogue-to-find-the-products","text":"To list available scenes covering an AOI between a range of time, the Catalogue ) will use the fields geometry , start_time , end_time and scene_type . The scene_type defines the consellation and the parameters depends on the catalogue:","title":"Parameters to request the catalogue to find the products"},{"location":"user-guide/payload/#copernicus","text":"Available scene_type/parameters : platformname : e.g. SENTINEL-1 , SENTINEL-2 productType : e.g. SLC , S2MSI1C filename For Sentinel-1 only: polarisationmode : default VV VH sensoroperationalmode : default IW relativeorbitnumber For optical only: cloudcoverpercentage : format [min TO max]","title":"Copernicus"},{"location":"user-guide/payload/#landsat","text":"Available scene_type/parameters : cloudcoverpercentage : format [min TO max]","title":"Landsat"},{"location":"user-guide/payload/#oneatlas","text":"OneAtlas available parameters: platform : e.g. PHR1B acquisitionIdentifier : e.g. ACQ_PNEO4_02426805581362 productType : e.g. bundle processingLevel cloudCover : e.g. [0,10] incidenceAngle workspace relation NB: To use other parameters or mix different kind of imagery. You need to group all your images in the same workspace (via OneAtlas) and reference only workspace in payload ingestion. For more information see: OneAtlas API guide","title":"OneAtlas"},{"location":"user-guide/payload/#parameters-to-download-and-process-the-products","text":"scene_graph_name defines a path to a processing graph that will be used by the downloader to pre-process the products (example: extract every burst from scenes). tile_graph_name defines a path to a processing graph that will be used by the processor to process the tiles. graph_config defines pairs of key-values that can be used by the preceding graphs (see Graph:Args )","title":"Parameters to download and process the products"},{"location":"user-guide/payload/#sentinel1","text":"scene_graph_name is usually used to extract the bursts from the Scenes. This step is done by the downloader as a pre-processing (1 job per Scene). tile_graph_name defines the processing to do for every Tiles. This step is done by the processor (1 job per Tile).","title":"Sentinel1"},{"location":"user-guide/payload/#sentinel2-spot-phr","text":"scene_graph_name can be used to pre-process data (example: extract Panchromatic & MultiSpectral Image from DIMAP product) but it usually used to copy the data to the ingester storage ( =CopyToStorage ). In this case a Tile is the whole Scene. tile_graph_name defines the processing to do for each Tile. This step is done by the processor. cf. Payload ingestion","title":"Sentinel2, SPOT, PHR"},{"location":"user-guide/payload/#parameters-to-index-the-output-products","text":"record_tags will be used by the catalogue, just before the ingestion starts, to create the records in the Geocube to index the output products layers will define the match between the output layers of the graph and the variables (and instances) of the Geocube (to index the data).","title":"Parameters to index the output products"},{"location":"user-guide/payload/#example-of-an-aoi-payload","text":"{ \"name\":\"DenmarkDemo\", \"type\":\"Feature\", \"geometry\":{ \"type\": \"MultiPolygon\", \"coordinates\": [ [ [ [10.061230468750068, 54.88637695312502], [9.957128906249977, 54.87246093750002], [9.903906250000063, 54.896630859374994], [9.80625, 54.90600585937503], [9.77119140625004, 55.059912109375034], [9.78125, 55.06904296875001], [9.830371093750015, 55.05825195312505], [9.998828125000045, 54.986474609374994], [10.05771484375006, 54.90791015624998], [10.061230468750068, 54.88637695312502] ] ] ] }, \"start_time\":\"2022-01-01T00:00:00.000Z\", \"end_time\":\"2022-02-10T00:00:00.000Z\", \"scene_type\":{ \"constellation\":\"sentinel1\", \"parameters\": { \"producttype\": \"SLC\", \"polarisationmode\": \"VV VH\", \"sensoroperationalmode\": \"IW\", \"relativeorbitnumber\": \"44\" } }, \"scene_graph_name\":\"S1Preprocessing\", \"tile_graph_name\":\"S1BackscatterCoherence\", \"graph_config\":{ \"projection\":\"EPSG:32632\", \"snap_cpu_parallelism\":\"8\", \"bkg_resampling\": \"BISINC_5_POINT_INTERPOLATION\" }, \"layers\":{ \"sigma0_VV\": {\"variable\":\"BackscatterSigma0VV\", \"instance\":\"RNKell\"}, \"sigma0_VH\": {\"variable\":\"BackscatterSigma0VH\", \"instance\":\"RNKell\"}, \"coh_VV\": {\"variable\":\"CoherenceVV\", \"instance\":\"master\"}, \"coh_VH\": {\"variable\":\"CoherenceVH\", \"instance\":\"master\"} }, \"record_tags\":{ \"source\": \"tutorial\", \"provider\": \"geocube-ingester\", \"area\":\"Denmark\" } }","title":"Example of an AOI Payload"},{"location":"user-guide/providers/","text":"Provider NB: This documentation is for user that want to use the providers. For documentation on how to implement a new provider, see Developer-Guide/Providers . Providers are implemented in order to download scenes. They are called one by one until the corresponding image is found. Copernicus : Sentinel scenes Creodias : Sentinel scenes GCS : any scenes stored in GCS (can be used to retrieve the annotations of an Sentinel1 archive stored in GCS) Local : any scenes stored locally OneAtlas : Airbus scenes (SPOT, Pleiades, PNEO) ASF : sentinel1 & 2 scenes Landsat AWS : Landsat 8&9 The scenes to be downloaded are sent to the Downloader Service, then the tiles to be processed are sent to the Processor Service. If an autoscaler is configured, the downloading and the processing are done in parallel using all available machines. Creodias Creodias account credentials are needed. creodias-username and creodias-password workflow and downloader arguments must be defined. https://finder.creodias.eu/resto/api/collections/<constellation>/search.json endpoint is use to request Creodias. and https://auth.creodias.eu/auth/realms/DIAS/protocol/openid-connect/token in order to get JWT Token. For more information see: Creodias Documentation Copernicus Copernicus account credentials are needed. copernicus-username and copernicus-password downloader arguments must be defined. For more information see: Copernicus Documentation GCS The downloader service must have the rights to read files on buckets. gs-provider-buckets workflow and downloader arguments must be defined. List of constellation:bucket comma-separated. bucket can contain several {IDENTIFIER} than will be replaced according to the sceneName. IDENTIFIER must be one of SCENE, MISSION_ID, PRODUCT_LEVEL, DATE(YEAR/MONTH/DAY), TIME(HOUR/MINUTE/SECOND), PDGS, ORBIT, TILE (LATITUDE_BAND/GRID_SQUARE/GRANULE_ID) For more information see: GCS Documentation Local directory No credentials needed. local-path downloader argument must be defined (local path where images are stored) OneAtlas OneAtlas account credentials are needed. oneatlas-username and oneatlas-apikey workflow and downloader arguments must be defined. By default, OneAtlas uses: https://access.foundation.api.oneatlas.airbus.com/api/v1/items as download endpoint https://data.api.oneatlas.airbus.com as order endpoint https://authenticate.foundation.api.oneatlas.airbus.com/auth/realms/IDP/protocol/openid-connect/token as authentication endpoint It's possible to use another endpoint by defining oneatlas-download-endpoint , oneatlas-order-endpoint and oneatlas-auth-endpoint . Important: concerning pricing, OneAtlas provider will be process and download image while credits are available. For more information see: OneAtlas Documentation ASF Asf account credentials are needed. asf-token downloader argument must be defined. Asf uses https://datapool.asf.alaska.edu/SLC/S{MISSION_VERSION}/{SCENE}.zip for SLC product and https://datapool.asf.alaska.edu/GRD-HD/S{MISSION_VERSION}/{SCENE}.zip for GRD products. ASF Documentation Landsat AWS AWS credentials are needed (pay-on-request): - --landsat-aws-access-key-id : Landsat AWS access key id - --landsat-aws-secret-access-key : Landsat AWS secret access key Landsat AWS","title":"Provider"},{"location":"user-guide/providers/#provider","text":"NB: This documentation is for user that want to use the providers. For documentation on how to implement a new provider, see Developer-Guide/Providers . Providers are implemented in order to download scenes. They are called one by one until the corresponding image is found. Copernicus : Sentinel scenes Creodias : Sentinel scenes GCS : any scenes stored in GCS (can be used to retrieve the annotations of an Sentinel1 archive stored in GCS) Local : any scenes stored locally OneAtlas : Airbus scenes (SPOT, Pleiades, PNEO) ASF : sentinel1 & 2 scenes Landsat AWS : Landsat 8&9 The scenes to be downloaded are sent to the Downloader Service, then the tiles to be processed are sent to the Processor Service. If an autoscaler is configured, the downloading and the processing are done in parallel using all available machines.","title":"Provider"},{"location":"user-guide/providers/#creodias","text":"Creodias account credentials are needed. creodias-username and creodias-password workflow and downloader arguments must be defined. https://finder.creodias.eu/resto/api/collections/<constellation>/search.json endpoint is use to request Creodias. and https://auth.creodias.eu/auth/realms/DIAS/protocol/openid-connect/token in order to get JWT Token. For more information see: Creodias Documentation","title":"Creodias"},{"location":"user-guide/providers/#copernicus","text":"Copernicus account credentials are needed. copernicus-username and copernicus-password downloader arguments must be defined. For more information see: Copernicus Documentation","title":"Copernicus"},{"location":"user-guide/providers/#gcs","text":"The downloader service must have the rights to read files on buckets. gs-provider-buckets workflow and downloader arguments must be defined. List of constellation:bucket comma-separated. bucket can contain several {IDENTIFIER} than will be replaced according to the sceneName. IDENTIFIER must be one of SCENE, MISSION_ID, PRODUCT_LEVEL, DATE(YEAR/MONTH/DAY), TIME(HOUR/MINUTE/SECOND), PDGS, ORBIT, TILE (LATITUDE_BAND/GRID_SQUARE/GRANULE_ID) For more information see: GCS Documentation","title":"GCS"},{"location":"user-guide/providers/#local-directory","text":"No credentials needed. local-path downloader argument must be defined (local path where images are stored)","title":"Local directory"},{"location":"user-guide/providers/#oneatlas","text":"OneAtlas account credentials are needed. oneatlas-username and oneatlas-apikey workflow and downloader arguments must be defined. By default, OneAtlas uses: https://access.foundation.api.oneatlas.airbus.com/api/v1/items as download endpoint https://data.api.oneatlas.airbus.com as order endpoint https://authenticate.foundation.api.oneatlas.airbus.com/auth/realms/IDP/protocol/openid-connect/token as authentication endpoint It's possible to use another endpoint by defining oneatlas-download-endpoint , oneatlas-order-endpoint and oneatlas-auth-endpoint . Important: concerning pricing, OneAtlas provider will be process and download image while credits are available. For more information see: OneAtlas Documentation","title":"OneAtlas"},{"location":"user-guide/providers/#asf","text":"Asf account credentials are needed. asf-token downloader argument must be defined. Asf uses https://datapool.asf.alaska.edu/SLC/S{MISSION_VERSION}/{SCENE}.zip for SLC product and https://datapool.asf.alaska.edu/GRD-HD/S{MISSION_VERSION}/{SCENE}.zip for GRD products. ASF Documentation","title":"ASF"},{"location":"user-guide/providers/#landsat-aws","text":"AWS credentials are needed (pay-on-request): - --landsat-aws-access-key-id : Landsat AWS access key id - --landsat-aws-secret-access-key : Landsat AWS secret access key Landsat AWS","title":"Landsat AWS"},{"location":"user-guide/run/","text":"Run a payload Workflow steps Catalogue generates a list of scenes matching with geometry , start_time , end_time and scene_type filter. Catalogue generates a list of tiles for every scene (For Sentinel1: Burst = Tiles, for other constellation Scenes = Tiles) Then, the ingestion can starts: Catalogue checks Geocube parameters validity (ie. layers JSON block which is reference Geocube variables and instances to use: must be existed) Catalogue creates associated records. If a record already exists (including the record_tags ), it is reused. Workflow is started, Downloader will start one job per Scene. After that, Processor will start also one job per Scene/Tile. List the available scenes The first step of the ingestion is to list the scenes available on the AOI at the given dates. The ingester will query the scenes from the external catalogues configured in the Catalogue Service. The Catalogue service has the endpoint /catalog/scenes ( GET or POST ) that takes a payload in input curl -F \"area=@{payloadFile}\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/scenes NB: This request supports (0-based) page/limit parameters to limit the query if the area or the date interval is big : /catalog/scenes?page={page}&limit={limit} . A limit of 1000 scenes is appropriate. This request returns a geojson file containing a list of features. Each feature is a product and has the following properties: aoi : name of the AOI, copied from payload.name data : used by the ingester. Some fields ( graph_config , graph_name , is_retriable , storage_uri ) are copied from the payload . Others are: date : of acquisition of the image record_id : id of the record created with wkt , date and tags (ignored at this stage) metadata : dictionary of metadata that can be used by the ingester (such as download_link ) uuid : unique id of the image provided by the catalogue id : unique id given to the scene by the ingester-workflow (ignored at this stage) source_id : id of the image tags : tags of the record with which the images will be indexed (dictionary of key:value). The record will be created at the begining of the ingestion wkt : a WKT of the image extent in EPSG:4326 Example: { \"aoi\":\"DenmarkDemoS2\", \"data\":{ \"date\":\"2022-01-04T10:34:31Z\", \"graph_config\":{}, \"graph_name\":\"CopyProductToStorage\", \"is_retriable\":true, \"record_id\":\"\", \"storage_uri\":\"\", \"uuid\":\"875f96fb-e591-4bcf-8202-fada69733e26\" }, \"id\":0, \"source_id\":\"S2A_MSIL1C_20220104T103431_N0510_R108_T32UNG_20240423T092858\", \"tags\":{ \"area\":\"Denmark\", \"cloudCoverPercentage\":\"50.9439586971957\", \"constellation\":\"SENTINEL2\", \"ingestionDate\":\"2022-01-04T10:34:31.000000Z\", \"orbit\":\"34139\", \"orbitDirection\":\"\", \"productType\":\"S2MSI1C\", \"provider\":\"geocube-ingester\", \"relativeOrbit\":\"108\",\"satellite\":\"SENTINEL2A\", \"source\":\"tutorial\",\"sourceID\":\"S2A_MSIL1C_20220104T103431_N0510_R108_T32UNG_20240423T092858\", \"uuid\":\"875f96fb-e591-4bcf-8202-fada69733e26\" }, \"wkt\":\"POLYGON ((8.999680177 55.89488809,8.999687664 54.95909887,10.71398549 54.94701782,10.7572757 55.93320247,9.0247385 55.94555573,8.999680177 55.89488809))\" } List the available tiles If the scenes are to be divided in tiles (Sentinel-1 bursts for example), the endpoint /catalog/tiles will do it. curl -F \"area=@{payloadFile}\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/tiles Start the ingestion The endpoint catalog/aoi ( POST ) lists the availables scenes and tiles then starts the ingestion of a payload . curl -F \"area=@{payloadFile}\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/aoi If the scenes or the tiles are already available (from a call of /catalog/scenes or /catalog/tiles ), the results can be sent to the end point, preventing the ingester to call the catalogue again. It's highly recommended to do so, by listing the scenes first and checking the results (optionaly editing them). From a list of tiles: curl -F \"area=@{payloadFile}\" -F \"tiles=@outputs/tiles.json\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/aoi Example of tiles.json: here From a list of scenes: curl -F \"area=@{payloadFile}\" -F \"scenes=@outputs/scenes.json\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/aoi Example of scenes.json: here .","title":"Run a payload"},{"location":"user-guide/run/#run-a-payload","text":"","title":"Run a payload"},{"location":"user-guide/run/#workflow-steps","text":"Catalogue generates a list of scenes matching with geometry , start_time , end_time and scene_type filter. Catalogue generates a list of tiles for every scene (For Sentinel1: Burst = Tiles, for other constellation Scenes = Tiles) Then, the ingestion can starts: Catalogue checks Geocube parameters validity (ie. layers JSON block which is reference Geocube variables and instances to use: must be existed) Catalogue creates associated records. If a record already exists (including the record_tags ), it is reused. Workflow is started, Downloader will start one job per Scene. After that, Processor will start also one job per Scene/Tile.","title":"Workflow steps"},{"location":"user-guide/run/#list-the-available-scenes","text":"The first step of the ingestion is to list the scenes available on the AOI at the given dates. The ingester will query the scenes from the external catalogues configured in the Catalogue Service. The Catalogue service has the endpoint /catalog/scenes ( GET or POST ) that takes a payload in input curl -F \"area=@{payloadFile}\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/scenes NB: This request supports (0-based) page/limit parameters to limit the query if the area or the date interval is big : /catalog/scenes?page={page}&limit={limit} . A limit of 1000 scenes is appropriate. This request returns a geojson file containing a list of features. Each feature is a product and has the following properties: aoi : name of the AOI, copied from payload.name data : used by the ingester. Some fields ( graph_config , graph_name , is_retriable , storage_uri ) are copied from the payload . Others are: date : of acquisition of the image record_id : id of the record created with wkt , date and tags (ignored at this stage) metadata : dictionary of metadata that can be used by the ingester (such as download_link ) uuid : unique id of the image provided by the catalogue id : unique id given to the scene by the ingester-workflow (ignored at this stage) source_id : id of the image tags : tags of the record with which the images will be indexed (dictionary of key:value). The record will be created at the begining of the ingestion wkt : a WKT of the image extent in EPSG:4326 Example: { \"aoi\":\"DenmarkDemoS2\", \"data\":{ \"date\":\"2022-01-04T10:34:31Z\", \"graph_config\":{}, \"graph_name\":\"CopyProductToStorage\", \"is_retriable\":true, \"record_id\":\"\", \"storage_uri\":\"\", \"uuid\":\"875f96fb-e591-4bcf-8202-fada69733e26\" }, \"id\":0, \"source_id\":\"S2A_MSIL1C_20220104T103431_N0510_R108_T32UNG_20240423T092858\", \"tags\":{ \"area\":\"Denmark\", \"cloudCoverPercentage\":\"50.9439586971957\", \"constellation\":\"SENTINEL2\", \"ingestionDate\":\"2022-01-04T10:34:31.000000Z\", \"orbit\":\"34139\", \"orbitDirection\":\"\", \"productType\":\"S2MSI1C\", \"provider\":\"geocube-ingester\", \"relativeOrbit\":\"108\",\"satellite\":\"SENTINEL2A\", \"source\":\"tutorial\",\"sourceID\":\"S2A_MSIL1C_20220104T103431_N0510_R108_T32UNG_20240423T092858\", \"uuid\":\"875f96fb-e591-4bcf-8202-fada69733e26\" }, \"wkt\":\"POLYGON ((8.999680177 55.89488809,8.999687664 54.95909887,10.71398549 54.94701782,10.7572757 55.93320247,9.0247385 55.94555573,8.999680177 55.89488809))\" }","title":"List the available scenes"},{"location":"user-guide/run/#list-the-available-tiles","text":"If the scenes are to be divided in tiles (Sentinel-1 bursts for example), the endpoint /catalog/tiles will do it. curl -F \"area=@{payloadFile}\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/tiles","title":"List the available tiles"},{"location":"user-guide/run/#start-the-ingestion","text":"The endpoint catalog/aoi ( POST ) lists the availables scenes and tiles then starts the ingestion of a payload . curl -F \"area=@{payloadFile}\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/aoi If the scenes or the tiles are already available (from a call of /catalog/scenes or /catalog/tiles ), the results can be sent to the end point, preventing the ingester to call the catalogue again. It's highly recommended to do so, by listing the scenes first and checking the results (optionaly editing them). From a list of tiles: curl -F \"area=@{payloadFile}\" -F \"tiles=@outputs/tiles.json\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/aoi Example of tiles.json: here From a list of scenes: curl -F \"area=@{payloadFile}\" -F \"scenes=@outputs/scenes.json\" -H \"Authorization: Bearer {token}\" {workflow_server}/catalog/aoi Example of scenes.json: here .","title":"Start the ingestion"},{"location":"user-guide/tutorials/","text":"Tutorials The Data Ingestion Jupyter notebook explains step-by-step how to: create a payload for ingestion send it to the ingester monitor the ingestion","title":"Tutorials"},{"location":"user-guide/tutorials/#tutorials","text":"The Data Ingestion Jupyter notebook explains step-by-step how to: create a payload for ingestion send it to the ingester monitor the ingestion","title":"Tutorials"}]}